{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AtomGen Documentation","text":"<p>Welcome to the documentation for AtomGen, a toolkit for atomistic machine learning and generative modeling. AtomGen provides researchers and developers with tools to explore, experiment, and innovate in the realm of molecular and materials science using state-of-the-art deep learning techniques.</p>"},{"location":"#overview","title":"Overview","text":"<p>AtomGen offers a comprehensive framework for handling atomistic datasets, training various models, and experimenting with different pre-training and fine-tuning tasks. It streamlines the process of working with diverse molecular and materials datasets, enabling large-scale pre-training and task-specific fine-tuning on atomistic data.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Data Handling: Efficient processing and loading of large-scale atomistic datasets.</li> <li>Model Architectures: Implementation of advanced models like AtomFormer, designed for molecular representation learning.</li> <li>Pre-training: Support for various pre-training tasks such as Structure to Energy and Forces (S2EF) prediction.</li> <li>Fine-tuning: Easy adaptation of pre-trained models to downstream tasks using ATOM3D benchmarks.</li> <li>Scalability: Designed for distributed training on multiple GPUs.</li> </ul>"},{"location":"#datasets","title":"Datasets","text":"<p>AtomGen supports a variety of datasets, including:</p> <ul> <li>S2EF-15M: A large-scale dataset aggregated from multiple sources (OC20, OC22, ODAC23, MPtrj, SPICE) for pre-training.</li> <li>ATOM3D Benchmarks: Task-specific datasets for molecular property prediction, including:</li> <li>SMP (Small Molecule Properties)</li> <li>PPI (Protein-Protein Interfaces)</li> <li>RES (Residue Identity)</li> <li>MSP (Mutation Stability Prediction)</li> <li>LBA (Ligand Binding Affinity)</li> <li>LEP (Ligand Efficacy Prediction)</li> <li>PSR (Protein Structure Ranking)</li> <li>RSR (RNA Structure Ranking)</li> </ul>"},{"location":"#models","title":"Models","text":"<p>The implemented model architectures in AtomGen is:</p> <ul> <li>AtomFormer: A transformer encoder model adapted for atomistic data, leveraging 3D spatial information.</li> <li>SchNet: A continuous-filter convolutional neural network for modeling quantum interactions.</li> <li>TokenGT: Tokenized graph transformer that treats all nodes and edges as independent tokens.</li> </ul> <p>The pre-trained models are based on AtomFormer, which can be fine-tuned on ATOM3D benchmarks for specific molecular property predictions.</p>"},{"location":"#tasks","title":"Tasks","text":"<p>AtomGen facilitates various tasks in molecular machine learning:</p> <ul> <li>Structure to Energy &amp; Forces (S2EF): Predicting energies and forces for atomistic systems.</li> <li>Masked Atom Modeling (MAM): Self-supervised learning by masking and predicting atom properties.</li> <li>Coordinate Denoising: Improving structural predictions by denoising perturbed coordinates.</li> <li>Downstream Tasks: Fine-tuning on ATOM3D benchmarks for specific molecular property predictions.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To get started with AtomGen, check out our User Guide for installation instructions, basic usage examples, and more detailed information on training and inference.</p> <p>For a deep dive into the API, explore the reference documentation for the API Reference.</p> <p>AtomGen is designed to be user-friendly while providing powerful capabilities for atomistic machine learning. Whether you're conducting research, developing new models, or applying machine learning to molecular systems, AtomGen provides a versatile toolkit to support your work.</p>"},{"location":"api/","title":"API Reference","text":"<p>Data module for the AtomGen library.</p> <p>This module contains the data classes and functions for pre-processing and collating data for training/inference.</p> <p>Models module for the AtomGen library.</p> <p>This module contains the model classes and functions for training and inference.</p>"},{"location":"api/#atomgen.data.data_collator","title":"data_collator","text":"<p>Data collator for atom modeling.</p>"},{"location":"api/#atomgen.data.data_collator.DataCollatorForAtomModeling","title":"DataCollatorForAtomModeling  <code>dataclass</code>","text":"<p>               Bases: <code>DataCollatorMixin</code></p> <p>Data collator used for atom modeling tasks in molecular representations.</p> <p>This collator prepares input data for various atom modeling tasks, including masked atom modeling (MAM), autoregressive modeling, and coordinate perturbation. It supports both padding and flattening of input data.</p> <p>Args:     tokenizer (PreTrainedTokenizer): Tokenizer used for encoding the data.     mam (Union[bool, float]): If True, uses original masked atom modeling.                               If float, masks a constant fraction of atoms/tokens.     autoregressive (bool): Whether to use autoregressive modeling.     coords_perturb (float): Standard deviation for coordinate perturbation.     return_lap_pe (bool): Whether to return Laplacian positional encoding.     return_edge_indices (bool): Whether to return edge indices.     k (int): Number of eigenvectors to use for Laplacian positional encoding.     max_radius (float): Maximum distance for edge cutoff.     max_neighbors (int): Maximum number of neighbors.     pad (bool): Whether to pad the input data.     pad_to_multiple_of (Optional[int]): Pad to multiple of this value.     return_tensors (str): Return tensors as \"pt\" or \"tf\".</p> <p>Attributes:</p> Name Type Description <code>tokenizer (PreTrainedTokenizer)</code> <code>The tokenizer used for encoding.</code> <p>mam (Union[bool, float]): The masked atom modeling setting. autoregressive (bool): The autoregressive modeling setting. coords_perturb (float): The coordinate perturbation standard deviation. return_lap_pe (bool): The Laplacian positional encoding setting. return_edge_indices (bool): The edge indices return setting. k (int): The number of eigenvectors for Laplacian PE. max_radius (float): The maximum distance for edge cutoff. max_neighbors (int): The maximum number of neighbors. pad (bool): The padding setting. pad_to_multiple_of (Optional[int]): The multiple for padding. return_tensors (str): The tensor return format.</p> Source code in <code>atomgen/data/data_collator.py</code> <pre><code>@dataclass\nclass DataCollatorForAtomModeling(DataCollatorMixin):  # type: ignore\n    \"\"\"\n    Data collator used for atom modeling tasks in molecular representations.\n\n    This collator prepares input data for various atom modeling tasks, including\n    masked atom modeling (MAM), autoregressive modeling, and coordinate perturbation.\n    It supports both padding and flattening of input data.\n\n    Args:\n        tokenizer (PreTrainedTokenizer): Tokenizer used for encoding the data.\n        mam (Union[bool, float]): If True, uses original masked atom modeling.\n                                  If float, masks a constant fraction of atoms/tokens.\n        autoregressive (bool): Whether to use autoregressive modeling.\n        coords_perturb (float): Standard deviation for coordinate perturbation.\n        return_lap_pe (bool): Whether to return Laplacian positional encoding.\n        return_edge_indices (bool): Whether to return edge indices.\n        k (int): Number of eigenvectors to use for Laplacian positional encoding.\n        max_radius (float): Maximum distance for edge cutoff.\n        max_neighbors (int): Maximum number of neighbors.\n        pad (bool): Whether to pad the input data.\n        pad_to_multiple_of (Optional[int]): Pad to multiple of this value.\n        return_tensors (str): Return tensors as \"pt\" or \"tf\".\n\n    Attributes\n    ----------\n        tokenizer (PreTrainedTokenizer): The tokenizer used for encoding.\n        mam (Union[bool, float]): The masked atom modeling setting.\n        autoregressive (bool): The autoregressive modeling setting.\n        coords_perturb (float): The coordinate perturbation standard deviation.\n        return_lap_pe (bool): The Laplacian positional encoding setting.\n        return_edge_indices (bool): The edge indices return setting.\n        k (int): The number of eigenvectors for Laplacian PE.\n        max_radius (float): The maximum distance for edge cutoff.\n        max_neighbors (int): The maximum number of neighbors.\n        pad (bool): The padding setting.\n        pad_to_multiple_of (Optional[int]): The multiple for padding.\n        return_tensors (str): The tensor return format.\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizer\n    mam: Union[bool, float] = True\n    autoregressive: bool = False\n    coords_perturb: float = 0.0\n    return_lap_pe: bool = False\n    return_edge_indices: bool = False\n    k: int = 16\n    max_radius: float = 12.0\n    max_neighbors: int = 20\n    pad: bool = True\n    pad_to_multiple_of: Optional[int] = None\n    return_tensors: str = \"pt\"\n\n    # ruff: noqa: PLR0912\n    def torch_call(\n        self, examples: List[Union[List[int], Any, Dict[str, Any]]]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Collate a batch of samples.\n\n        Args:\n            examples: List of samples to collate.\n\n        Returns\n        -------\n            Dict[str, Any]: Dictionary of batched data.\n\n        \"\"\"\n        # Handle dict or lists with proper padding and conversion to tensor.\n        if self.pad:\n            if isinstance(examples[0], Mapping):\n                batch: Dict[str, Any] = self.tokenizer.pad(\n                    examples,\n                    return_tensors=\"pt\",\n                    pad_to_multiple_of=self.pad_to_multiple_of,\n                )\n            else:\n                batch = {\n                    \"input_ids\": _torch_collate_batch(\n                        examples,\n                        self.tokenizer,\n                        pad_to_multiple_of=self.pad_to_multiple_of,\n                    )\n                }\n\n            if self.return_lap_pe:\n                # Compute Laplacian and positional encoding\n                (\n                    batch[\"node_pe\"],\n                    batch[\"edge_pe\"],\n                    batch[\"attention_mask\"],\n                ) = self.torch_compute_lap_pe(batch[\"coords\"], batch[\"attention_mask\"])\n            if self.return_edge_indices:\n                # Compute edge indices and distances\n                (\n                    batch[\"edge_indices\"],\n                    batch[\"edge_distances\"],\n                    batch[\"attention_mask\"],\n                ) = self.torch_compute_edges(batch[\"coords\"], batch[\"attention_mask\"])\n        else:\n            # flatten all lists in examples and concatenate\n            batch = self.flatten_batch(examples)\n\n        t = torch.zeros(batch[\"input_ids\"].shape[0]).float().uniform_(0, 1)\n        t = torch.cos(t * math.pi * 0.5)\n\n        if self.mam:\n            special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n            if special_tokens_mask is None:\n                special_tokens_mask = [\n                    self.tokenizer.get_special_tokens_mask(\n                        val, already_has_special_tokens=True\n                    )\n                    for val in batch[\"input_ids\"].tolist()\n                ]\n                special_tokens_mask = torch.tensor(\n                    special_tokens_mask, dtype=torch.bool\n                )\n            else:\n                special_tokens_mask = special_tokens_mask.bool()\n\n            if isinstance(self.mam, float):\n                # Constant masking of a float fraction of the atoms/tokens\n                mask = torch.bernoulli(\n                    torch.full(batch[\"input_ids\"].shape, self.mam)\n                ).bool()\n                batch[\"input_ids\"], batch[\"labels\"] = self.apply_mask(\n                    batch[\"input_ids\"], mask, special_tokens_mask\n                )\n            else:\n                # Original MaskGIT functionality\n                batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n                    batch[\"input_ids\"], t, special_tokens_mask=special_tokens_mask\n                )\n\n        if self.autoregressive:\n            # extend coords\n            batch[\"coords\"] = torch.cat(\n                [\n                    torch.zeros_like(batch[\"coords\"][:, :1]),\n                    batch[\"coords\"],\n                    torch.zeros_like(batch[\"coords\"][:, :1]),\n                ],\n                dim=1,\n            )\n            if \"labels\" not in batch:\n                batch[\"labels\"] = batch[\"input_ids\"].clone()\n                batch[\"labels_coords\"] = batch[\"coords\"].clone()\n\n            # create mask of ~special_tokens_mask and exclude bos and eos tokens\n            special_tokens_mask[batch[\"labels\"] == self.tokenizer.bos_token_id] = False\n            special_tokens_mask[batch[\"labels\"] == self.tokenizer.eos_token_id] = False\n            batch[\"labels\"] = torch.where(~special_tokens_mask, batch[\"labels\"], -100)\n\n        if self.coords_perturb &gt; 0:\n            batch[\"coords\"], batch[\"labels_coords\"] = self.torch_perturb_coords(\n                batch[\"coords\"],\n                batch.get(\"fixed\", None),\n                self.coords_perturb,\n            )\n\n        return batch\n\n    def torch_mask_tokens(\n        self, inputs: Any, t: Any, special_tokens_mask: Optional[Any] = None\n    ) -&gt; Tuple[Any, Any]:\n        \"\"\"Prepare masked tokens inputs/labels for masked atom modeling.\"\"\"\n        labels = inputs.clone()\n\n        batch, seq_len = inputs.shape\n        num_token_masked = (seq_len * t).round().clamp(min=1)\n        batch_randperm = torch.rand((batch, seq_len)).argsort(dim=-1)\n        mask = batch_randperm &lt; num_token_masked.unsqueeze(1)\n        inputs = torch.where(\n            ~mask,\n            inputs,\n            self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token),\n        )\n        labels = torch.where(mask, labels, -100)\n        if special_tokens_mask is not None:\n            labels = torch.where(~special_tokens_mask, labels, -100)\n\n        return inputs, labels\n\n    def apply_mask(\n        self,\n        inputs: torch.Tensor,\n        mask: torch.Tensor,\n        special_tokens_mask: torch.Tensor,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Apply the mask to the input tokens.\"\"\"\n        labels = inputs.clone()\n        inputs = torch.where(\n            mask,\n            torch.tensor(self.tokenizer.mask_token_id, device=inputs.device),\n            inputs,\n        )\n        labels = torch.where(\n            ~mask | special_tokens_mask,\n            torch.tensor(-100, device=labels.device),\n            labels,\n        )\n        return inputs, labels\n\n    def torch_perturb_coords(\n        self, inputs: torch.Tensor, fixed: Optional[torch.Tensor], perturb_std: float\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Prepare perturbed coords inputs/labels for coordinate denoising.\"\"\"\n        if fixed is None:\n            fixed = torch.zeros_like(inputs).bool()\n        labels = inputs.clone()\n        noise = torch.empty_like(inputs).normal_(0, perturb_std)\n        inputs[~fixed.bool()] += noise[~fixed.bool()]\n        return inputs, labels\n\n    def flatten_batch(self, examples: Any) -&gt; Dict[str, Any]:\n        \"\"\"Flatten all lists in examples and concatenate with batch indicator.\"\"\"\n        batch = {}\n        for key in examples[0]:\n            if key == \"input_ids\":\n                lengths = []\n                for sample in examples:\n                    lengths.append(len(sample[key]))\n                batch[\"batch\"] = torch.arange(len(examples)).repeat_interleave(\n                    torch.tensor(lengths)\n                )\n                batch[key] = torch.cat(\n                    [torch.tensor(sample[key]) for sample in examples], dim=0\n                )\n            elif (\n                key.startswith(\"coords\")\n                or key.endswith(\"coords\")\n                or (key.startswith(\"fixed\") or key.endswith(\"fixed\"))\n            ):\n                batch[key] = torch.cat(\n                    [torch.tensor(sample[key]) for sample in examples], dim=0\n                )\n            elif key.startswith(\"energy\") or key.endswith(\"energy\"):\n                batch[key] = torch.tensor([sample[key] for sample in examples])\n            elif key.startswith(\"forces\") or key.endswith(\"forces\"):\n                batch[key] = torch.cat(\n                    [torch.tensor(sample[key]) for sample in examples], dim=0\n                )\n        return batch\n\n    def torch_compute_edges(self, coords: Any, attention_mask: Any) -&gt; Any:\n        \"\"\"Compute edge indices and distances for each batch.\"\"\"\n        dist_matrix = torch.cdist(coords, coords, p=2)\n        b, n, _ = dist_matrix.shape\n\n        # ignore distance in padded coords by setting to large number\n        attention_mask_mult = (1.0 - attention_mask) * 1e6\n        dist_matrix = dist_matrix + attention_mask_mult.unsqueeze(1)\n        dist_matrix = dist_matrix + attention_mask_mult.unsqueeze(2)\n\n        # to avoid self-loop, set diagonal to a large number\n        dist_matrix = dist_matrix + torch.eye(n) * 1e6\n\n        # get adjacency matrix using cutoff\n        adjacency_matrix = torch.where(dist_matrix &lt;= self.max_radius, 1, 0).float()\n\n        # set max_num_neighbors to 20 to get closest 20 neighbors and set rest to zero\n        _, topk_indices = torch.topk(\n            dist_matrix,\n            k=min(self.max_neighbors, dist_matrix.size(2)),\n            dim=2,\n            largest=False,\n        )\n        mask = torch.zeros_like(dist_matrix)\n        mask.scatter_(2, topk_indices, 1)\n        adjacency_matrix *= mask\n\n        # get distances for each batch in for loop\n        distance_list = []\n        for bi in range(b):\n            distance = dist_matrix[bi][adjacency_matrix[bi] != 0]\n            distance_list.append(distance)\n\n        # get edge_indices for each batch in for loop\n        edge_indices_list = []\n        lengths = []\n        for bi in range(b):\n            edge_indices = torch.column_stack(torch.where(adjacency_matrix[bi] != 0))\n            lengths.append(edge_indices.size(0))\n            edge_indices_list.append(edge_indices)\n\n        edge_indices = pad_sequence(\n            edge_indices_list, batch_first=True, padding_value=0\n        )\n        edge_distances = pad_sequence(distance_list, batch_first=True, padding_value=-1)\n        edge_attention_mask = torch.cat(\n            [\n                torch.cat(\n                    [\n                        torch.ones(1, length),\n                        torch.zeros(1, edge_indices.size(1) - length),\n                    ],\n                    dim=1,\n                )\n                for length in lengths\n            ],\n            dim=0,\n        )\n        attention_mask = torch.cat([attention_mask, edge_attention_mask], dim=1)\n\n        return edge_indices, edge_distances, attention_mask\n\n    def torch_compute_lap_pe(self, coords: Any, attention_mask: Any) -&gt; Any:\n        \"\"\"Compute Laplacian positional encoding for each batch.\"\"\"\n        dist_matrix = torch.cdist(coords, coords, p=2)\n        b, n, _ = dist_matrix.shape\n\n        # ignore distance in padded coords by setting to large number\n        attention_mask_mult = (1.0 - attention_mask) * 1e6\n        dist_matrix = dist_matrix + attention_mask_mult.unsqueeze(1)\n        dist_matrix = dist_matrix + attention_mask_mult.unsqueeze(2)\n\n        # to avoid self-loop, set diagonal to a large number\n        dist_matrix = dist_matrix + torch.eye(n) * 1e6\n\n        # get adjacency matrix using cutoff\n        adjacency_matrix = torch.where(dist_matrix &lt;= self.max_radius, 1, 0).float()\n\n        # set max_num_neighbors to 20 to get closest 20 neighbors and set rest to zero\n        _, topk_indices = torch.topk(\n            dist_matrix,\n            k=min(self.max_neighbors, dist_matrix.size(2)),\n            dim=2,\n            largest=False,\n        )\n        mask = torch.zeros_like(dist_matrix)\n        mask.scatter_(2, topk_indices, 1)\n        adjacency_matrix *= mask\n\n        # get distances for each batch in for loop\n        distance_list = []\n        for bi in range(b):\n            distance = dist_matrix[bi][adjacency_matrix[bi] != 0]\n            distance_list.append(distance)\n\n        # get edge_indices for each batch in for loop\n        edge_indices_list = []\n        for bi in range(b):\n            edge_indices = torch.column_stack(torch.where(adjacency_matrix[bi] != 0))\n            edge_indices_list.append(edge_indices)\n\n        # Construct graph Laplacian for each batch\n        degree_matrix = torch.diag_embed(adjacency_matrix.sum(dim=2).clip(1) ** -0.5)\n        laplacian_matrix = (\n            torch.eye(n) - degree_matrix @ adjacency_matrix @ degree_matrix\n        )\n\n        # Eigenvalue decomposition for each batch\n        eigval, eigvec = torch.linalg.eigh(laplacian_matrix)\n\n        eigvec = eigvec.float()  # [N, N (channels)]\n        eigval = torch.sort(torch.abs(torch.real(eigval)))[0].float()  # [N (channels),]\n\n        if eigvec.size(1) &lt; self.k:\n            node_pe = f.pad(eigvec, (0, self.k - eigvec.size(2), 0, 0))\n        else:\n            # use smallest eigenvalues\n            node_pe = eigvec[:, :, : self.k]\n\n        all_edges_pe_list = []\n        lengths = []\n        for i, edge_indices in enumerate(edge_indices_list):\n            e = edge_indices.shape[0]\n            lengths.append(e)\n            all_edges_pe = torch.zeros([e, 2 * self.k])\n            all_edges_pe[:, : self.k] = torch.index_select(\n                node_pe[i], 0, edge_indices[:, 0]\n            )\n            all_edges_pe[:, self.k :] = torch.index_select(\n                node_pe[i], 0, edge_indices[:, 1]\n            )\n            all_edges_pe_list.append(all_edges_pe)\n\n        # get attention mask for edge_pe based on all_edges_pe_list\n\n        edge_pe = pad_sequence(all_edges_pe_list, batch_first=True, padding_value=0)\n        edge_attention_mask = torch.cat(\n            [\n                torch.cat(\n                    [torch.ones(1, length), torch.zeros(1, edge_pe.size(1) - length)],\n                    dim=1,\n                )\n                for length in lengths\n            ],\n            dim=0,\n        )\n        attention_mask = torch.cat([attention_mask, edge_attention_mask], dim=1)\n\n        edge_distances = pad_sequence(distance_list, batch_first=True, padding_value=-1)\n        edge_pe = torch.cat([edge_pe, edge_distances.unsqueeze(-1)], dim=2)\n\n        node_pe = torch.cat([node_pe, node_pe], dim=2)\n\n        return node_pe, edge_pe, attention_mask\n</code></pre>"},{"location":"api/#atomgen.data.data_collator.DataCollatorForAtomModeling.torch_call","title":"torch_call","text":"<pre><code>torch_call(examples)\n</code></pre> <p>Collate a batch of samples.</p> <p>Args:     examples: List of samples to collate.</p> <p>Returns:</p> Type Description <code>    Dict[str, Any]: Dictionary of batched data.</code> Source code in <code>atomgen/data/data_collator.py</code> <pre><code>def torch_call(\n    self, examples: List[Union[List[int], Any, Dict[str, Any]]]\n) -&gt; Dict[str, Any]:\n    \"\"\"Collate a batch of samples.\n\n    Args:\n        examples: List of samples to collate.\n\n    Returns\n    -------\n        Dict[str, Any]: Dictionary of batched data.\n\n    \"\"\"\n    # Handle dict or lists with proper padding and conversion to tensor.\n    if self.pad:\n        if isinstance(examples[0], Mapping):\n            batch: Dict[str, Any] = self.tokenizer.pad(\n                examples,\n                return_tensors=\"pt\",\n                pad_to_multiple_of=self.pad_to_multiple_of,\n            )\n        else:\n            batch = {\n                \"input_ids\": _torch_collate_batch(\n                    examples,\n                    self.tokenizer,\n                    pad_to_multiple_of=self.pad_to_multiple_of,\n                )\n            }\n\n        if self.return_lap_pe:\n            # Compute Laplacian and positional encoding\n            (\n                batch[\"node_pe\"],\n                batch[\"edge_pe\"],\n                batch[\"attention_mask\"],\n            ) = self.torch_compute_lap_pe(batch[\"coords\"], batch[\"attention_mask\"])\n        if self.return_edge_indices:\n            # Compute edge indices and distances\n            (\n                batch[\"edge_indices\"],\n                batch[\"edge_distances\"],\n                batch[\"attention_mask\"],\n            ) = self.torch_compute_edges(batch[\"coords\"], batch[\"attention_mask\"])\n    else:\n        # flatten all lists in examples and concatenate\n        batch = self.flatten_batch(examples)\n\n    t = torch.zeros(batch[\"input_ids\"].shape[0]).float().uniform_(0, 1)\n    t = torch.cos(t * math.pi * 0.5)\n\n    if self.mam:\n        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n        if special_tokens_mask is None:\n            special_tokens_mask = [\n                self.tokenizer.get_special_tokens_mask(\n                    val, already_has_special_tokens=True\n                )\n                for val in batch[\"input_ids\"].tolist()\n            ]\n            special_tokens_mask = torch.tensor(\n                special_tokens_mask, dtype=torch.bool\n            )\n        else:\n            special_tokens_mask = special_tokens_mask.bool()\n\n        if isinstance(self.mam, float):\n            # Constant masking of a float fraction of the atoms/tokens\n            mask = torch.bernoulli(\n                torch.full(batch[\"input_ids\"].shape, self.mam)\n            ).bool()\n            batch[\"input_ids\"], batch[\"labels\"] = self.apply_mask(\n                batch[\"input_ids\"], mask, special_tokens_mask\n            )\n        else:\n            # Original MaskGIT functionality\n            batch[\"input_ids\"], batch[\"labels\"] = self.torch_mask_tokens(\n                batch[\"input_ids\"], t, special_tokens_mask=special_tokens_mask\n            )\n\n    if self.autoregressive:\n        # extend coords\n        batch[\"coords\"] = torch.cat(\n            [\n                torch.zeros_like(batch[\"coords\"][:, :1]),\n                batch[\"coords\"],\n                torch.zeros_like(batch[\"coords\"][:, :1]),\n            ],\n            dim=1,\n        )\n        if \"labels\" not in batch:\n            batch[\"labels\"] = batch[\"input_ids\"].clone()\n            batch[\"labels_coords\"] = batch[\"coords\"].clone()\n\n        # create mask of ~special_tokens_mask and exclude bos and eos tokens\n        special_tokens_mask[batch[\"labels\"] == self.tokenizer.bos_token_id] = False\n        special_tokens_mask[batch[\"labels\"] == self.tokenizer.eos_token_id] = False\n        batch[\"labels\"] = torch.where(~special_tokens_mask, batch[\"labels\"], -100)\n\n    if self.coords_perturb &gt; 0:\n        batch[\"coords\"], batch[\"labels_coords\"] = self.torch_perturb_coords(\n            batch[\"coords\"],\n            batch.get(\"fixed\", None),\n            self.coords_perturb,\n        )\n\n    return batch\n</code></pre>"},{"location":"api/#atomgen.data.data_collator.DataCollatorForAtomModeling.torch_mask_tokens","title":"torch_mask_tokens","text":"<pre><code>torch_mask_tokens(inputs, t, special_tokens_mask=None)\n</code></pre> <p>Prepare masked tokens inputs/labels for masked atom modeling.</p> Source code in <code>atomgen/data/data_collator.py</code> <pre><code>def torch_mask_tokens(\n    self, inputs: Any, t: Any, special_tokens_mask: Optional[Any] = None\n) -&gt; Tuple[Any, Any]:\n    \"\"\"Prepare masked tokens inputs/labels for masked atom modeling.\"\"\"\n    labels = inputs.clone()\n\n    batch, seq_len = inputs.shape\n    num_token_masked = (seq_len * t).round().clamp(min=1)\n    batch_randperm = torch.rand((batch, seq_len)).argsort(dim=-1)\n    mask = batch_randperm &lt; num_token_masked.unsqueeze(1)\n    inputs = torch.where(\n        ~mask,\n        inputs,\n        self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token),\n    )\n    labels = torch.where(mask, labels, -100)\n    if special_tokens_mask is not None:\n        labels = torch.where(~special_tokens_mask, labels, -100)\n\n    return inputs, labels\n</code></pre>"},{"location":"api/#atomgen.data.data_collator.DataCollatorForAtomModeling.apply_mask","title":"apply_mask","text":"<pre><code>apply_mask(inputs, mask, special_tokens_mask)\n</code></pre> <p>Apply the mask to the input tokens.</p> Source code in <code>atomgen/data/data_collator.py</code> <pre><code>def apply_mask(\n    self,\n    inputs: torch.Tensor,\n    mask: torch.Tensor,\n    special_tokens_mask: torch.Tensor,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Apply the mask to the input tokens.\"\"\"\n    labels = inputs.clone()\n    inputs = torch.where(\n        mask,\n        torch.tensor(self.tokenizer.mask_token_id, device=inputs.device),\n        inputs,\n    )\n    labels = torch.where(\n        ~mask | special_tokens_mask,\n        torch.tensor(-100, device=labels.device),\n        labels,\n    )\n    return inputs, labels\n</code></pre>"},{"location":"api/#atomgen.data.data_collator.DataCollatorForAtomModeling.torch_perturb_coords","title":"torch_perturb_coords","text":"<pre><code>torch_perturb_coords(inputs, fixed, perturb_std)\n</code></pre> <p>Prepare perturbed coords inputs/labels for coordinate denoising.</p> Source code in <code>atomgen/data/data_collator.py</code> <pre><code>def torch_perturb_coords(\n    self, inputs: torch.Tensor, fixed: Optional[torch.Tensor], perturb_std: float\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Prepare perturbed coords inputs/labels for coordinate denoising.\"\"\"\n    if fixed is None:\n        fixed = torch.zeros_like(inputs).bool()\n    labels = inputs.clone()\n    noise = torch.empty_like(inputs).normal_(0, perturb_std)\n    inputs[~fixed.bool()] += noise[~fixed.bool()]\n    return inputs, labels\n</code></pre>"},{"location":"api/#atomgen.data.data_collator.DataCollatorForAtomModeling.flatten_batch","title":"flatten_batch","text":"<pre><code>flatten_batch(examples)\n</code></pre> <p>Flatten all lists in examples and concatenate with batch indicator.</p> Source code in <code>atomgen/data/data_collator.py</code> <pre><code>def flatten_batch(self, examples: Any) -&gt; Dict[str, Any]:\n    \"\"\"Flatten all lists in examples and concatenate with batch indicator.\"\"\"\n    batch = {}\n    for key in examples[0]:\n        if key == \"input_ids\":\n            lengths = []\n            for sample in examples:\n                lengths.append(len(sample[key]))\n            batch[\"batch\"] = torch.arange(len(examples)).repeat_interleave(\n                torch.tensor(lengths)\n            )\n            batch[key] = torch.cat(\n                [torch.tensor(sample[key]) for sample in examples], dim=0\n            )\n        elif (\n            key.startswith(\"coords\")\n            or key.endswith(\"coords\")\n            or (key.startswith(\"fixed\") or key.endswith(\"fixed\"))\n        ):\n            batch[key] = torch.cat(\n                [torch.tensor(sample[key]) for sample in examples], dim=0\n            )\n        elif key.startswith(\"energy\") or key.endswith(\"energy\"):\n            batch[key] = torch.tensor([sample[key] for sample in examples])\n        elif key.startswith(\"forces\") or key.endswith(\"forces\"):\n            batch[key] = torch.cat(\n                [torch.tensor(sample[key]) for sample in examples], dim=0\n            )\n    return batch\n</code></pre>"},{"location":"api/#atomgen.data.data_collator.DataCollatorForAtomModeling.torch_compute_edges","title":"torch_compute_edges","text":"<pre><code>torch_compute_edges(coords, attention_mask)\n</code></pre> <p>Compute edge indices and distances for each batch.</p> Source code in <code>atomgen/data/data_collator.py</code> <pre><code>def torch_compute_edges(self, coords: Any, attention_mask: Any) -&gt; Any:\n    \"\"\"Compute edge indices and distances for each batch.\"\"\"\n    dist_matrix = torch.cdist(coords, coords, p=2)\n    b, n, _ = dist_matrix.shape\n\n    # ignore distance in padded coords by setting to large number\n    attention_mask_mult = (1.0 - attention_mask) * 1e6\n    dist_matrix = dist_matrix + attention_mask_mult.unsqueeze(1)\n    dist_matrix = dist_matrix + attention_mask_mult.unsqueeze(2)\n\n    # to avoid self-loop, set diagonal to a large number\n    dist_matrix = dist_matrix + torch.eye(n) * 1e6\n\n    # get adjacency matrix using cutoff\n    adjacency_matrix = torch.where(dist_matrix &lt;= self.max_radius, 1, 0).float()\n\n    # set max_num_neighbors to 20 to get closest 20 neighbors and set rest to zero\n    _, topk_indices = torch.topk(\n        dist_matrix,\n        k=min(self.max_neighbors, dist_matrix.size(2)),\n        dim=2,\n        largest=False,\n    )\n    mask = torch.zeros_like(dist_matrix)\n    mask.scatter_(2, topk_indices, 1)\n    adjacency_matrix *= mask\n\n    # get distances for each batch in for loop\n    distance_list = []\n    for bi in range(b):\n        distance = dist_matrix[bi][adjacency_matrix[bi] != 0]\n        distance_list.append(distance)\n\n    # get edge_indices for each batch in for loop\n    edge_indices_list = []\n    lengths = []\n    for bi in range(b):\n        edge_indices = torch.column_stack(torch.where(adjacency_matrix[bi] != 0))\n        lengths.append(edge_indices.size(0))\n        edge_indices_list.append(edge_indices)\n\n    edge_indices = pad_sequence(\n        edge_indices_list, batch_first=True, padding_value=0\n    )\n    edge_distances = pad_sequence(distance_list, batch_first=True, padding_value=-1)\n    edge_attention_mask = torch.cat(\n        [\n            torch.cat(\n                [\n                    torch.ones(1, length),\n                    torch.zeros(1, edge_indices.size(1) - length),\n                ],\n                dim=1,\n            )\n            for length in lengths\n        ],\n        dim=0,\n    )\n    attention_mask = torch.cat([attention_mask, edge_attention_mask], dim=1)\n\n    return edge_indices, edge_distances, attention_mask\n</code></pre>"},{"location":"api/#atomgen.data.data_collator.DataCollatorForAtomModeling.torch_compute_lap_pe","title":"torch_compute_lap_pe","text":"<pre><code>torch_compute_lap_pe(coords, attention_mask)\n</code></pre> <p>Compute Laplacian positional encoding for each batch.</p> Source code in <code>atomgen/data/data_collator.py</code> <pre><code>def torch_compute_lap_pe(self, coords: Any, attention_mask: Any) -&gt; Any:\n    \"\"\"Compute Laplacian positional encoding for each batch.\"\"\"\n    dist_matrix = torch.cdist(coords, coords, p=2)\n    b, n, _ = dist_matrix.shape\n\n    # ignore distance in padded coords by setting to large number\n    attention_mask_mult = (1.0 - attention_mask) * 1e6\n    dist_matrix = dist_matrix + attention_mask_mult.unsqueeze(1)\n    dist_matrix = dist_matrix + attention_mask_mult.unsqueeze(2)\n\n    # to avoid self-loop, set diagonal to a large number\n    dist_matrix = dist_matrix + torch.eye(n) * 1e6\n\n    # get adjacency matrix using cutoff\n    adjacency_matrix = torch.where(dist_matrix &lt;= self.max_radius, 1, 0).float()\n\n    # set max_num_neighbors to 20 to get closest 20 neighbors and set rest to zero\n    _, topk_indices = torch.topk(\n        dist_matrix,\n        k=min(self.max_neighbors, dist_matrix.size(2)),\n        dim=2,\n        largest=False,\n    )\n    mask = torch.zeros_like(dist_matrix)\n    mask.scatter_(2, topk_indices, 1)\n    adjacency_matrix *= mask\n\n    # get distances for each batch in for loop\n    distance_list = []\n    for bi in range(b):\n        distance = dist_matrix[bi][adjacency_matrix[bi] != 0]\n        distance_list.append(distance)\n\n    # get edge_indices for each batch in for loop\n    edge_indices_list = []\n    for bi in range(b):\n        edge_indices = torch.column_stack(torch.where(adjacency_matrix[bi] != 0))\n        edge_indices_list.append(edge_indices)\n\n    # Construct graph Laplacian for each batch\n    degree_matrix = torch.diag_embed(adjacency_matrix.sum(dim=2).clip(1) ** -0.5)\n    laplacian_matrix = (\n        torch.eye(n) - degree_matrix @ adjacency_matrix @ degree_matrix\n    )\n\n    # Eigenvalue decomposition for each batch\n    eigval, eigvec = torch.linalg.eigh(laplacian_matrix)\n\n    eigvec = eigvec.float()  # [N, N (channels)]\n    eigval = torch.sort(torch.abs(torch.real(eigval)))[0].float()  # [N (channels),]\n\n    if eigvec.size(1) &lt; self.k:\n        node_pe = f.pad(eigvec, (0, self.k - eigvec.size(2), 0, 0))\n    else:\n        # use smallest eigenvalues\n        node_pe = eigvec[:, :, : self.k]\n\n    all_edges_pe_list = []\n    lengths = []\n    for i, edge_indices in enumerate(edge_indices_list):\n        e = edge_indices.shape[0]\n        lengths.append(e)\n        all_edges_pe = torch.zeros([e, 2 * self.k])\n        all_edges_pe[:, : self.k] = torch.index_select(\n            node_pe[i], 0, edge_indices[:, 0]\n        )\n        all_edges_pe[:, self.k :] = torch.index_select(\n            node_pe[i], 0, edge_indices[:, 1]\n        )\n        all_edges_pe_list.append(all_edges_pe)\n\n    # get attention mask for edge_pe based on all_edges_pe_list\n\n    edge_pe = pad_sequence(all_edges_pe_list, batch_first=True, padding_value=0)\n    edge_attention_mask = torch.cat(\n        [\n            torch.cat(\n                [torch.ones(1, length), torch.zeros(1, edge_pe.size(1) - length)],\n                dim=1,\n            )\n            for length in lengths\n        ],\n        dim=0,\n    )\n    attention_mask = torch.cat([attention_mask, edge_attention_mask], dim=1)\n\n    edge_distances = pad_sequence(distance_list, batch_first=True, padding_value=-1)\n    edge_pe = torch.cat([edge_pe, edge_distances.unsqueeze(-1)], dim=2)\n\n    node_pe = torch.cat([node_pe, node_pe], dim=2)\n\n    return node_pe, edge_pe, attention_mask\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer","title":"tokenizer","text":"<p>tokenization module for atom modeling.</p>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer","title":"AtomTokenizer","text":"<p>               Bases: <code>PreTrainedTokenizer</code></p> <p>Tokenizer for atomistic data.</p> <p>Args:     vocab_file: The path to the vocabulary file.     pad_token: The padding token.     mask_token: The mask token.     bos_token: The beginning of system token.     eos_token: The end of system token.     cls_token: The classification token.     kwargs: Additional keyword arguments.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>class AtomTokenizer(PreTrainedTokenizer):  # type: ignore[misc]\n    \"\"\"\n    Tokenizer for atomistic data.\n\n    Args:\n        vocab_file: The path to the vocabulary file.\n        pad_token: The padding token.\n        mask_token: The mask token.\n        bos_token: The beginning of system token.\n        eos_token: The end of system token.\n        cls_token: The classification token.\n        kwargs: Additional keyword arguments.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_file: str,\n        pad_token: str = \"&lt;pad&gt;\",\n        mask_token: str = \"&lt;mask&gt;\",\n        bos_token: str = \"&lt;bos&gt;\",\n        eos_token: str = \"&lt;eos&gt;\",\n        cls_token: str = \"&lt;graph&gt;\",\n        **kwargs: Dict[str, Union[bool, str, PaddingStrategy]],\n    ) -&gt; None:\n        self.vocab: Dict[str, int] = self.load_vocab(vocab_file)\n        self.ids_to_tokens: Dict[int, str] = collections.OrderedDict(\n            [(ids, tok) for tok, ids in self.vocab.items()]\n        )\n\n        super().__init__(\n            pad_token=pad_token,\n            mask_token=mask_token,\n            bos_token=bos_token,\n            eos_token=eos_token,\n            cls_token=cls_token,\n            **kwargs,\n        )\n\n    @staticmethod\n    def load_vocab(vocab_file: str) -&gt; Dict[str, int]:\n        \"\"\"Load the vocabulary from a json file.\"\"\"\n        with open(vocab_file, \"r\") as f:\n            vocab = json.load(f)\n            if not isinstance(vocab, dict):\n                raise ValueError(\n                    \"The vocabulary file is not a json file or is not formatted correctly.\"\n                )\n        return vocab\n\n    def _tokenize(self, text: str) -&gt; List[str]:\n        \"\"\"Tokenize the text.\"\"\"\n        tokens = []\n        i = 0\n        while i &lt; len(text):\n            if i + 1 &lt; len(text) and text[i : i + 2] in self.vocab:\n                tokens.append(text[i : i + 2])\n                i += 2\n            else:\n                tokens.append(text[i])\n                i += 1\n        return tokens\n\n    def _convert_token_to_id(self, token: str) -&gt; int:\n        \"\"\"Convert the chemical symbols to atomic numbers.\"\"\"\n        return self.vocab[token]\n\n    def _convert_id_to_token(self, index: int) -&gt; str:\n        return self.ids_to_tokens[index]\n\n    def get_vocab(self) -&gt; Dict[str, int]:\n        \"\"\"Get the vocabulary.\"\"\"\n        return self.vocab\n\n    def get_vocab_size(self) -&gt; int:\n        \"\"\"Get the size of the vocabulary.\"\"\"\n        return len(self.vocab)\n\n    def convert_tokens_to_string(self, tokens: List[str]) -&gt; str:\n        \"\"\"Convert the list of chemical symbol tokens to a concatenated string.\"\"\"\n        return \"\".join(tokens)\n\n    def pad(\n        self,\n        encoded_inputs: Union[\n            BatchEncoding,\n            List[BatchEncoding],\n            Dict[str, EncodedInput],\n            Dict[str, List[EncodedInput]],\n            List[Dict[str, EncodedInput]],\n        ],\n        padding: Union[bool, str, PaddingStrategy] = True,\n        max_length: Optional[int] = None,\n        pad_to_multiple_of: Optional[int] = None,\n        return_attention_mask: Optional[bool] = None,\n        return_tensors: Optional[Union[str, TensorType]] = None,\n        verbose: bool = True,\n    ) -&gt; BatchEncoding:\n        \"\"\"Pad the input data.\"\"\"\n        if isinstance(encoded_inputs, list):\n            if isinstance(encoded_inputs[0], Mapping):\n                if any(\n                    key.startswith(\"coords\") or key.endswith(\"coords\")\n                    for key in encoded_inputs[0]\n                ):\n                    encoded_inputs = self.pad_coords(\n                        encoded_inputs,\n                        max_length=max_length,\n                        pad_to_multiple_of=pad_to_multiple_of,\n                    )\n                if any(\n                    key.startswith(\"forces\") or key.endswith(\"forces\")\n                    for key in encoded_inputs[0]\n                ):\n                    encoded_inputs = self.pad_forces(\n                        encoded_inputs,\n                        max_length=max_length,\n                        pad_to_multiple_of=pad_to_multiple_of,\n                    )\n                if any(\n                    key.startswith(\"fixed\") or key.endswith(\"fixed\")\n                    for key in encoded_inputs[0]\n                ):\n                    encoded_inputs = self.pad_fixed(\n                        encoded_inputs,\n                        max_length=max_length,\n                        pad_to_multiple_of=pad_to_multiple_of,\n                    )\n        elif isinstance(encoded_inputs, Mapping):\n            if any(\"coords\" in key for key in encoded_inputs):\n                encoded_inputs = self.pad_coords(\n                    encoded_inputs,\n                    max_length=max_length,\n                    pad_to_multiple_of=pad_to_multiple_of,\n                )\n            if any(\"fixed\" in key for key in encoded_inputs):\n                encoded_inputs = self.pad_fixed(\n                    encoded_inputs,\n                    max_length=max_length,\n                    pad_to_multiple_of=pad_to_multiple_of,\n                )\n\n        return super().pad(\n            encoded_inputs=encoded_inputs,\n            padding=padding,\n            max_length=max_length,\n            pad_to_multiple_of=pad_to_multiple_of,\n            return_attention_mask=return_attention_mask,\n            return_tensors=return_tensors,\n            verbose=verbose,\n        )\n\n    def pad_coords(\n        self,\n        batch: Union[Mapping, List[Mapping]],\n        max_length: Optional[int] = None,\n        pad_to_multiple_of: Optional[int] = None,\n    ) -&gt; Union[Mapping, List[Mapping]]:\n        \"\"\"Pad the coordinates to the same length.\"\"\"\n        if isinstance(batch, Mapping):\n            coord_keys = [\n                key\n                for key in batch\n                if key.startswith(\"coords\") or key.endswith(\"coords\")\n            ]\n        elif isinstance(batch, list):\n            coord_keys = [\n                key\n                for key in batch[0]\n                if key.startswith(\"coords\") or key.endswith(\"coords\")\n            ]\n        for key in coord_keys:\n            if isinstance(batch, Mapping):\n                coords = batch[key]\n            elif isinstance(batch, list):\n                coords = [sample[key] for sample in batch]\n            max_length = (\n                max([len(c) for c in coords]) if max_length is None else max_length\n            )\n            if pad_to_multiple_of is not None and max_length % pad_to_multiple_of != 0:\n                max_length = (\n                    (max_length // pad_to_multiple_of) + 1\n                ) * pad_to_multiple_of\n            for c in coords:\n                c.extend([[0.0, 0.0, 0.0]] * (max_length - len(c)))\n            if isinstance(batch, list):\n                for i, sample in enumerate(batch):\n                    sample[key] = coords[i]\n        return batch\n\n    def pad_forces(\n        self,\n        batch: Union[Mapping, List[Mapping]],\n        max_length: Optional[int] = None,\n        pad_to_multiple_of: Optional[int] = None,\n    ) -&gt; Union[Mapping, List[Mapping]]:\n        \"\"\"Pad the forces to the same length.\"\"\"\n        if isinstance(batch, Mapping):\n            force_keys = [\n                key\n                for key in batch\n                if key.startswith(\"forces\") or key.endswith(\"forces\")\n            ]\n        elif isinstance(batch, list):\n            force_keys = [\n                key\n                for key in batch[0]\n                if key.startswith(\"forces\") or key.endswith(\"forces\")\n            ]\n        for key in force_keys:\n            if isinstance(batch, Mapping):\n                forces = batch[key]\n            elif isinstance(batch, list):\n                forces = [sample[key] for sample in batch]\n            max_length = (\n                max([len(c) for c in forces]) if max_length is None else max_length\n            )\n            if pad_to_multiple_of is not None and max_length % pad_to_multiple_of != 0:\n                max_length = (\n                    (max_length // pad_to_multiple_of) + 1\n                ) * pad_to_multiple_of\n            for f in forces:\n                f.extend([[0.0, 0.0, 0.0]] * (max_length - len(f)))\n            if isinstance(batch, list):\n                for i, sample in enumerate(batch):\n                    sample[key] = forces[i]\n        return batch\n\n    def pad_fixed(\n        self,\n        batch: Union[Mapping, List[Mapping]],\n        max_length: Optional[int] = None,\n        pad_to_multiple_of: Optional[int] = None,\n    ) -&gt; Union[Mapping, List[Mapping]]:\n        \"\"\"Pad the fixed mask to the same length.\"\"\"\n        if isinstance(batch, Mapping):\n            fixed_keys = [\n                key for key in batch if key.startswith(\"fixed\") or key.endswith(\"fixed\")\n            ]\n        elif isinstance(batch, list):\n            fixed_keys = [\n                key\n                for key in batch[0]\n                if key.startswith(\"fixed\") or key.endswith(\"fixed\")\n            ]\n        for key in fixed_keys:\n            if isinstance(batch, Mapping):\n                fixed = batch[key]\n            elif isinstance(batch, list):\n                fixed = [sample[key] for sample in batch]\n            max_length = (\n                max([len(c) for c in fixed]) if max_length is None else max_length\n            )\n            if pad_to_multiple_of is not None and max_length % pad_to_multiple_of != 0:\n                max_length = (\n                    (max_length // pad_to_multiple_of) + 1\n                ) * pad_to_multiple_of\n            for f in fixed:\n                f.extend([True] * (max_length - len(f)))\n            if isinstance(batch, list):\n                for i, sample in enumerate(batch):\n                    sample[key] = fixed[i]\n        return batch\n\n    def save_vocabulary(\n        self, save_directory: str, filename_prefix: Optional[str] = None\n    ) -&gt; Tuple[str]:\n        \"\"\"Save the vocabulary to a json file.\"\"\"\n        vocab_file = os.path.join(\n            save_directory,\n            (filename_prefix + \"-\" if filename_prefix else \"\")\n            + VOCAB_FILES_NAMES[\"vocab_file\"],\n        )\n        with open(vocab_file, \"w\") as f:\n            json.dump(self.vocab, f)\n        return (vocab_file,)\n\n    @classmethod\n    def from_pretrained(cls, *inputs: Any, **kwargs: Any) -&gt; Any:\n        \"\"\"Load the tokenizer from a pretrained model.\"\"\"\n        return super().from_pretrained(*inputs, **kwargs)\n\n    # add special tokens &lt;bos&gt; and &lt;eos&gt;\n    def build_inputs_with_special_tokens(\n        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n    ) -&gt; List[int]:\n        \"\"\"Build the input with special tokens.\"\"\"\n        bos = [self.bos_token_id]\n        eos = [self.eos_token_id]\n\n        if token_ids_1 is None:\n            return bos + token_ids_0 + eos\n        return bos + token_ids_0 + eos + token_ids_1 + eos\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.load_vocab","title":"load_vocab  <code>staticmethod</code>","text":"<pre><code>load_vocab(vocab_file)\n</code></pre> <p>Load the vocabulary from a json file.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>@staticmethod\ndef load_vocab(vocab_file: str) -&gt; Dict[str, int]:\n    \"\"\"Load the vocabulary from a json file.\"\"\"\n    with open(vocab_file, \"r\") as f:\n        vocab = json.load(f)\n        if not isinstance(vocab, dict):\n            raise ValueError(\n                \"The vocabulary file is not a json file or is not formatted correctly.\"\n            )\n    return vocab\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.get_vocab","title":"get_vocab","text":"<pre><code>get_vocab()\n</code></pre> <p>Get the vocabulary.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>def get_vocab(self) -&gt; Dict[str, int]:\n    \"\"\"Get the vocabulary.\"\"\"\n    return self.vocab\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.get_vocab_size","title":"get_vocab_size","text":"<pre><code>get_vocab_size()\n</code></pre> <p>Get the size of the vocabulary.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>def get_vocab_size(self) -&gt; int:\n    \"\"\"Get the size of the vocabulary.\"\"\"\n    return len(self.vocab)\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.convert_tokens_to_string","title":"convert_tokens_to_string","text":"<pre><code>convert_tokens_to_string(tokens)\n</code></pre> <p>Convert the list of chemical symbol tokens to a concatenated string.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>def convert_tokens_to_string(self, tokens: List[str]) -&gt; str:\n    \"\"\"Convert the list of chemical symbol tokens to a concatenated string.\"\"\"\n    return \"\".join(tokens)\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.pad","title":"pad","text":"<pre><code>pad(\n    encoded_inputs,\n    padding=True,\n    max_length=None,\n    pad_to_multiple_of=None,\n    return_attention_mask=None,\n    return_tensors=None,\n    verbose=True,\n)\n</code></pre> <p>Pad the input data.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>def pad(\n    self,\n    encoded_inputs: Union[\n        BatchEncoding,\n        List[BatchEncoding],\n        Dict[str, EncodedInput],\n        Dict[str, List[EncodedInput]],\n        List[Dict[str, EncodedInput]],\n    ],\n    padding: Union[bool, str, PaddingStrategy] = True,\n    max_length: Optional[int] = None,\n    pad_to_multiple_of: Optional[int] = None,\n    return_attention_mask: Optional[bool] = None,\n    return_tensors: Optional[Union[str, TensorType]] = None,\n    verbose: bool = True,\n) -&gt; BatchEncoding:\n    \"\"\"Pad the input data.\"\"\"\n    if isinstance(encoded_inputs, list):\n        if isinstance(encoded_inputs[0], Mapping):\n            if any(\n                key.startswith(\"coords\") or key.endswith(\"coords\")\n                for key in encoded_inputs[0]\n            ):\n                encoded_inputs = self.pad_coords(\n                    encoded_inputs,\n                    max_length=max_length,\n                    pad_to_multiple_of=pad_to_multiple_of,\n                )\n            if any(\n                key.startswith(\"forces\") or key.endswith(\"forces\")\n                for key in encoded_inputs[0]\n            ):\n                encoded_inputs = self.pad_forces(\n                    encoded_inputs,\n                    max_length=max_length,\n                    pad_to_multiple_of=pad_to_multiple_of,\n                )\n            if any(\n                key.startswith(\"fixed\") or key.endswith(\"fixed\")\n                for key in encoded_inputs[0]\n            ):\n                encoded_inputs = self.pad_fixed(\n                    encoded_inputs,\n                    max_length=max_length,\n                    pad_to_multiple_of=pad_to_multiple_of,\n                )\n    elif isinstance(encoded_inputs, Mapping):\n        if any(\"coords\" in key for key in encoded_inputs):\n            encoded_inputs = self.pad_coords(\n                encoded_inputs,\n                max_length=max_length,\n                pad_to_multiple_of=pad_to_multiple_of,\n            )\n        if any(\"fixed\" in key for key in encoded_inputs):\n            encoded_inputs = self.pad_fixed(\n                encoded_inputs,\n                max_length=max_length,\n                pad_to_multiple_of=pad_to_multiple_of,\n            )\n\n    return super().pad(\n        encoded_inputs=encoded_inputs,\n        padding=padding,\n        max_length=max_length,\n        pad_to_multiple_of=pad_to_multiple_of,\n        return_attention_mask=return_attention_mask,\n        return_tensors=return_tensors,\n        verbose=verbose,\n    )\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.pad_coords","title":"pad_coords","text":"<pre><code>pad_coords(batch, max_length=None, pad_to_multiple_of=None)\n</code></pre> <p>Pad the coordinates to the same length.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>def pad_coords(\n    self,\n    batch: Union[Mapping, List[Mapping]],\n    max_length: Optional[int] = None,\n    pad_to_multiple_of: Optional[int] = None,\n) -&gt; Union[Mapping, List[Mapping]]:\n    \"\"\"Pad the coordinates to the same length.\"\"\"\n    if isinstance(batch, Mapping):\n        coord_keys = [\n            key\n            for key in batch\n            if key.startswith(\"coords\") or key.endswith(\"coords\")\n        ]\n    elif isinstance(batch, list):\n        coord_keys = [\n            key\n            for key in batch[0]\n            if key.startswith(\"coords\") or key.endswith(\"coords\")\n        ]\n    for key in coord_keys:\n        if isinstance(batch, Mapping):\n            coords = batch[key]\n        elif isinstance(batch, list):\n            coords = [sample[key] for sample in batch]\n        max_length = (\n            max([len(c) for c in coords]) if max_length is None else max_length\n        )\n        if pad_to_multiple_of is not None and max_length % pad_to_multiple_of != 0:\n            max_length = (\n                (max_length // pad_to_multiple_of) + 1\n            ) * pad_to_multiple_of\n        for c in coords:\n            c.extend([[0.0, 0.0, 0.0]] * (max_length - len(c)))\n        if isinstance(batch, list):\n            for i, sample in enumerate(batch):\n                sample[key] = coords[i]\n    return batch\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.pad_forces","title":"pad_forces","text":"<pre><code>pad_forces(batch, max_length=None, pad_to_multiple_of=None)\n</code></pre> <p>Pad the forces to the same length.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>def pad_forces(\n    self,\n    batch: Union[Mapping, List[Mapping]],\n    max_length: Optional[int] = None,\n    pad_to_multiple_of: Optional[int] = None,\n) -&gt; Union[Mapping, List[Mapping]]:\n    \"\"\"Pad the forces to the same length.\"\"\"\n    if isinstance(batch, Mapping):\n        force_keys = [\n            key\n            for key in batch\n            if key.startswith(\"forces\") or key.endswith(\"forces\")\n        ]\n    elif isinstance(batch, list):\n        force_keys = [\n            key\n            for key in batch[0]\n            if key.startswith(\"forces\") or key.endswith(\"forces\")\n        ]\n    for key in force_keys:\n        if isinstance(batch, Mapping):\n            forces = batch[key]\n        elif isinstance(batch, list):\n            forces = [sample[key] for sample in batch]\n        max_length = (\n            max([len(c) for c in forces]) if max_length is None else max_length\n        )\n        if pad_to_multiple_of is not None and max_length % pad_to_multiple_of != 0:\n            max_length = (\n                (max_length // pad_to_multiple_of) + 1\n            ) * pad_to_multiple_of\n        for f in forces:\n            f.extend([[0.0, 0.0, 0.0]] * (max_length - len(f)))\n        if isinstance(batch, list):\n            for i, sample in enumerate(batch):\n                sample[key] = forces[i]\n    return batch\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.pad_fixed","title":"pad_fixed","text":"<pre><code>pad_fixed(batch, max_length=None, pad_to_multiple_of=None)\n</code></pre> <p>Pad the fixed mask to the same length.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>def pad_fixed(\n    self,\n    batch: Union[Mapping, List[Mapping]],\n    max_length: Optional[int] = None,\n    pad_to_multiple_of: Optional[int] = None,\n) -&gt; Union[Mapping, List[Mapping]]:\n    \"\"\"Pad the fixed mask to the same length.\"\"\"\n    if isinstance(batch, Mapping):\n        fixed_keys = [\n            key for key in batch if key.startswith(\"fixed\") or key.endswith(\"fixed\")\n        ]\n    elif isinstance(batch, list):\n        fixed_keys = [\n            key\n            for key in batch[0]\n            if key.startswith(\"fixed\") or key.endswith(\"fixed\")\n        ]\n    for key in fixed_keys:\n        if isinstance(batch, Mapping):\n            fixed = batch[key]\n        elif isinstance(batch, list):\n            fixed = [sample[key] for sample in batch]\n        max_length = (\n            max([len(c) for c in fixed]) if max_length is None else max_length\n        )\n        if pad_to_multiple_of is not None and max_length % pad_to_multiple_of != 0:\n            max_length = (\n                (max_length // pad_to_multiple_of) + 1\n            ) * pad_to_multiple_of\n        for f in fixed:\n            f.extend([True] * (max_length - len(f)))\n        if isinstance(batch, list):\n            for i, sample in enumerate(batch):\n                sample[key] = fixed[i]\n    return batch\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.save_vocabulary","title":"save_vocabulary","text":"<pre><code>save_vocabulary(save_directory, filename_prefix=None)\n</code></pre> <p>Save the vocabulary to a json file.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>def save_vocabulary(\n    self, save_directory: str, filename_prefix: Optional[str] = None\n) -&gt; Tuple[str]:\n    \"\"\"Save the vocabulary to a json file.\"\"\"\n    vocab_file = os.path.join(\n        save_directory,\n        (filename_prefix + \"-\" if filename_prefix else \"\")\n        + VOCAB_FILES_NAMES[\"vocab_file\"],\n    )\n    with open(vocab_file, \"w\") as f:\n        json.dump(self.vocab, f)\n    return (vocab_file,)\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.from_pretrained","title":"from_pretrained  <code>classmethod</code>","text":"<pre><code>from_pretrained(*inputs, **kwargs)\n</code></pre> <p>Load the tokenizer from a pretrained model.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>@classmethod\ndef from_pretrained(cls, *inputs: Any, **kwargs: Any) -&gt; Any:\n    \"\"\"Load the tokenizer from a pretrained model.\"\"\"\n    return super().from_pretrained(*inputs, **kwargs)\n</code></pre>"},{"location":"api/#atomgen.data.tokenizer.AtomTokenizer.build_inputs_with_special_tokens","title":"build_inputs_with_special_tokens","text":"<pre><code>build_inputs_with_special_tokens(\n    token_ids_0, token_ids_1=None\n)\n</code></pre> <p>Build the input with special tokens.</p> Source code in <code>atomgen/data/tokenizer.py</code> <pre><code>def build_inputs_with_special_tokens(\n    self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n) -&gt; List[int]:\n    \"\"\"Build the input with special tokens.\"\"\"\n    bos = [self.bos_token_id]\n    eos = [self.eos_token_id]\n\n    if token_ids_1 is None:\n        return bos + token_ids_0 + eos\n    return bos + token_ids_0 + eos + token_ids_1 + eos\n</code></pre>"},{"location":"api/#atomgen.data.utils","title":"utils","text":"<p>Utilities for data processing and evaluation.</p>"},{"location":"api/#atomgen.data.utils.compute_metrics_smp","title":"compute_metrics_smp","text":"<pre><code>compute_metrics_smp(eval_pred)\n</code></pre> <p>Compute MAE for 20 regression labels for the SMP task.</p> Source code in <code>atomgen/data/utils.py</code> <pre><code>def compute_metrics_smp(eval_pred: EvalPrediction) -&gt; Dict[str, Any]:\n    \"\"\"Compute MAE for 20 regression labels for the SMP task.\"\"\"\n    pred = eval_pred.predictions\n    label = eval_pred.label_ids\n\n    # get Mean absolute error for each 20 pred and labels\n    maes = {\n        \"rot_const_A\": None,\n        \"rot_const_B\": None,\n        \"rot_const_C\": None,\n        \"dipole_moment\": None,\n        \"isotropic_polarizability\": None,\n        \"HOMO\": None,\n        \"LUMO\": None,\n        \"gap\": None,\n        \"electronic_spatial_extent\": None,\n        \"zero_point_vib_energy\": None,\n        \"internal_energy_0K\": None,\n        \"internal_energy_298.15K\": None,\n        \"enthalpy_298.15K\": None,\n        \"free_energy_298.15K\": None,\n        \"heat_capacity_298.15K\": None,\n        \"thermochem_internal_energy_0K\": None,\n        \"thermochem_internal_energy_298.15K\": None,\n        \"thermochem_enthalpy_298.15K\": None,\n        \"thermochem_free_energy_298.15K\": None,\n        \"thermochem_heat_capacity_298.15K\": None,\n    }\n    for i in range(20):\n        value = np.mean(np.abs(pred[:, i] - label[:, i]))\n        maes[list(maes.keys())[i]] = value\n\n    return maes\n</code></pre>"},{"location":"api/#atomgen.data.utils.compute_metrics_ppi","title":"compute_metrics_ppi","text":"<pre><code>compute_metrics_ppi(eval_pred)\n</code></pre> <p>Compute AUROC for the PIP task.</p> Source code in <code>atomgen/data/utils.py</code> <pre><code>def compute_metrics_ppi(eval_pred: EvalPrediction) -&gt; Dict[str, Any]:\n    \"\"\"Compute AUROC for the PIP task.\"\"\"\n    pred = expit(eval_pred.predictions &gt; 0.5)\n    label = eval_pred.label_ids\n\n    # compute AUROC for each label\n    for i in range(20):\n        auroc = roc_auc_score(label[:, i], pred[:, i])\n\n    return {\"auroc\": auroc}\n</code></pre>"},{"location":"api/#atomgen.data.utils.compute_metrics_res","title":"compute_metrics_res","text":"<pre><code>compute_metrics_res(eval_pred)\n</code></pre> <p>Compute accuracy for the RES task.</p> Source code in <code>atomgen/data/utils.py</code> <pre><code>def compute_metrics_res(eval_pred: EvalPrediction) -&gt; Dict[str, Any]:\n    \"\"\"Compute accuracy for the RES task.\"\"\"\n    pred = softmax(eval_pred.predictions).argmax(axis=1)\n    label = eval_pred.label_ids\n\n    # compute accuracy\n\n    acc = accuracy_score(label, pred)\n\n    return {\"accuracy\": acc}\n</code></pre>"},{"location":"api/#atomgen.data.utils.compute_metrics_msp","title":"compute_metrics_msp","text":"<pre><code>compute_metrics_msp(eval_pred)\n</code></pre> <p>Compute AUROC for the MSP task.</p> Source code in <code>atomgen/data/utils.py</code> <pre><code>def compute_metrics_msp(eval_pred: EvalPrediction) -&gt; Dict[str, Any]:\n    \"\"\"Compute AUROC for the MSP task.\"\"\"\n    pred = eval_pred.predictions\n    label = eval_pred.label_ids\n\n    # compute AUROC for each label\n    auroc = roc_auc_score(label, pred)\n\n    return {\"auroc\": auroc}\n</code></pre>"},{"location":"api/#atomgen.data.utils.compute_metrics_lba","title":"compute_metrics_lba","text":"<pre><code>compute_metrics_lba(eval_pred)\n</code></pre> <p>Compute RMSE for the LBA task.</p> Source code in <code>atomgen/data/utils.py</code> <pre><code>def compute_metrics_lba(eval_pred: EvalPrediction) -&gt; Dict[str, Any]:\n    \"\"\"Compute RMSE for the LBA task.\"\"\"\n    pred = eval_pred.predictions\n    label = eval_pred.label_ids\n\n    # compute RMSE for each label\n    rmse = np.sqrt(np.mean((pred - label) ** 2))\n    global_pearson = pearsonr(pred.flatten(), label.flatten())[0]\n    global_spearman = spearmanr(pred.flatten(), label.flatten())[0]\n\n    return {\n        \"rmse\": rmse,\n        \"global_pearson\": global_pearson,\n        \"global_spearman\": global_spearman,\n    }\n</code></pre>"},{"location":"api/#atomgen.data.utils.compute_metrics_lep","title":"compute_metrics_lep","text":"<pre><code>compute_metrics_lep(eval_pred)\n</code></pre> <p>Compute AUROC for the LEP task.</p> Source code in <code>atomgen/data/utils.py</code> <pre><code>def compute_metrics_lep(eval_pred: EvalPrediction) -&gt; Dict[str, Any]:\n    \"\"\"Compute AUROC for the LEP task.\"\"\"\n    pred = expit(eval_pred.predictions) &gt; 0.5\n    label = eval_pred.label_ids\n\n    # compute AUROC for each label\n    auroc = roc_auc_score(label, pred)\n\n    return {\"auroc\": auroc}\n</code></pre>"},{"location":"api/#atomgen.data.utils.compute_metrics_psr","title":"compute_metrics_psr","text":"<pre><code>compute_metrics_psr(eval_pred)\n</code></pre> <p>Compute global spearman correlation for the PSR task.</p> Source code in <code>atomgen/data/utils.py</code> <pre><code>def compute_metrics_psr(eval_pred: EvalPrediction) -&gt; Dict[str, Any]:\n    \"\"\"Compute global spearman correlation for the PSR task.\"\"\"\n    pred = eval_pred.predictions\n    label = eval_pred.label_ids\n\n    # compute global spearman correlation\n    global_spearman = spearmanr(pred.flatten(), label.flatten())[0]\n\n    return {\"global_spearman\": global_spearman}\n</code></pre>"},{"location":"api/#atomgen.data.utils.compute_metrics_rsr","title":"compute_metrics_rsr","text":"<pre><code>compute_metrics_rsr(eval_pred)\n</code></pre> <p>Compute global spearman correlation for the RSR task.</p> Source code in <code>atomgen/data/utils.py</code> <pre><code>def compute_metrics_rsr(eval_pred: EvalPrediction) -&gt; Dict[str, Any]:\n    \"\"\"Compute global spearman correlation for the RSR task.\"\"\"\n    pred = eval_pred.predictions\n    label = eval_pred.label_ids\n\n    # compute global spearman correlation\n    global_spearman = spearmanr(pred.flatten(), label.flatten())[0]\n\n    return {\"global_spearman\": global_spearman}\n</code></pre>"},{"location":"api/#atomgen.models.configuration_atomformer","title":"configuration_atomformer","text":"<p>Configuration class for Atomformer.</p>"},{"location":"api/#atomgen.models.configuration_atomformer.AtomformerConfig","title":"AtomformerConfig","text":"<p>               Bases: <code>PretrainedConfig</code></p> <p>Configuration of a :class:<code>~transform:class:</code>~transformers.AtomformerModel`.</p> <p>It is used to instantiate an Atomformer model according to the specified arguments.</p> Source code in <code>atomgen/models/configuration_atomformer.py</code> <pre><code>class AtomformerConfig(PretrainedConfig):  # type: ignore\n    r\"\"\"\n    Configuration of a :class:`~transform:class:`~transformers.AtomformerModel`.\n\n    It is used to instantiate an Atomformer model according to the specified arguments.\n    \"\"\"\n\n    model_type = \"atomformer\"\n\n    def __init__(\n        self,\n        vocab_size: int = 123,\n        dim: int = 768,\n        num_heads: int = 32,\n        depth: int = 12,\n        mlp_ratio: int = 1,\n        k: int = 128,\n        dropout: float = 0.0,\n        mask_token_id: int = 0,\n        pad_token_id: int = 119,\n        bos_token_id: int = 120,\n        eos_token_id: int = 121,\n        cls_token_id: int = 122,\n        **kwargs: Any,\n    ) -&gt; None:\n        super().__init__(**kwargs)\n        self.vocab_size = vocab_size\n        self.dim = dim\n        self.num_heads = num_heads\n        self.depth = depth\n        self.mlp_ratio = mlp_ratio\n        self.k = k\n\n        self.dropout = dropout\n        self.mask_token_id = mask_token_id\n        self.pad_token_id = pad_token_id\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        self.cls_token_id = cls_token_id\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer","title":"modeling_atomformer","text":"<p>Implementation of the Atomformer model.</p>"},{"location":"api/#atomgen.models.modeling_atomformer.GaussianLayer","title":"GaussianLayer","text":"<p>               Bases: <code>Module</code></p> <p>Gaussian pairwise positional embedding layer.</p> <p>This layer computes the Gaussian positional embeddings for the pairwise distances between atoms in a molecule.</p> <p>Taken from: https://github.com/microsoft/Graphormer/blob/main/graphormer/models/graphormer_3d.py</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class GaussianLayer(nn.Module):\n    \"\"\"Gaussian pairwise positional embedding layer.\n\n    This layer computes the Gaussian positional embeddings for the pairwise distances\n    between atoms in a molecule.\n\n    Taken from: https://github.com/microsoft/Graphormer/blob/main/graphormer/models/graphormer_3d.py\n    \"\"\"\n\n    def __init__(self, k: int = 128, edge_types: int = 1024):\n        super().__init__()\n        self.k = k\n        self.means = nn.Embedding(1, k)\n        self.stds = nn.Embedding(1, k)\n        self.mul = nn.Embedding(edge_types, 1)\n        self.bias = nn.Embedding(edge_types, 1)\n        nn.init.uniform_(self.means.weight, 0, 3)\n        nn.init.uniform_(self.stds.weight, 0, 3)\n        nn.init.constant_(self.bias.weight, 0)\n        nn.init.constant_(self.mul.weight, 1)\n\n    def forward(self, x: torch.Tensor, edge_types: int) -&gt; torch.Tensor:\n        \"\"\"Forward pass to compute the Gaussian pos. embeddings.\"\"\"\n        mul = self.mul(edge_types)\n        bias = self.bias(edge_types)\n        x = mul * x.unsqueeze(-1) + bias\n        x = x.expand(-1, -1, -1, self.k)\n        mean = self.means.weight.float().view(-1)\n        std = self.stds.weight.float().view(-1).abs() + 1e-5\n        output: torch.Tensor = gaussian(x.float(), mean, std).type_as(self.means.weight)\n        return output\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.GaussianLayer.forward","title":"forward","text":"<pre><code>forward(x, edge_types)\n</code></pre> <p>Forward pass to compute the Gaussian pos. embeddings.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(self, x: torch.Tensor, edge_types: int) -&gt; torch.Tensor:\n    \"\"\"Forward pass to compute the Gaussian pos. embeddings.\"\"\"\n    mul = self.mul(edge_types)\n    bias = self.bias(edge_types)\n    x = mul * x.unsqueeze(-1) + bias\n    x = x.expand(-1, -1, -1, self.k)\n    mean = self.means.weight.float().view(-1)\n    std = self.stds.weight.float().view(-1).abs() + 1e-5\n    output: torch.Tensor = gaussian(x.float(), mean, std).type_as(self.means.weight)\n    return output\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.ParallelBlock","title":"ParallelBlock","text":"<p>               Bases: <code>Module</code></p> <p>Parallel transformer block (MLP &amp; Attention in parallel).</p> <p>Based on:   'Scaling Vision Transformers to 22 Billion Parameters` - https://arxiv.org/abs/2302.05442</p> <p>Adapted from TIMM implementation.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class ParallelBlock(nn.Module):\n    \"\"\"Parallel transformer block (MLP &amp; Attention in parallel).\n\n    Based on:\n      'Scaling Vision Transformers to 22 Billion Parameters` - https://arxiv.org/abs/2302.05442\n\n    Adapted from TIMM implementation.\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: int = 4,\n        dropout: float = 0.0,\n        k: int = 128,\n        gradient_checkpointing: bool = False,\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, (\n            f\"dim {dim} should be divisible by num_heads {num_heads}\"\n        )\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.mlp_hidden_dim = int(mlp_ratio * dim)\n        self.proj_drop = nn.Dropout(dropout)\n        self.attn_drop = nn.Dropout(dropout)\n        self.gradient_checkpointing = gradient_checkpointing\n\n        self.in_proj_in_dim = dim\n        self.in_proj_out_dim = self.mlp_hidden_dim + 3 * dim\n        self.out_proj_in_dim = self.mlp_hidden_dim + dim\n        self.out_proj_out_dim = 2 * dim\n\n        self.in_split = [self.mlp_hidden_dim] + [dim] * 3\n        self.out_split = [dim] * 2\n\n        self.in_norm = nn.LayerNorm(dim)\n        self.q_norm = nn.LayerNorm(self.head_dim)\n        self.k_norm = nn.LayerNorm(self.head_dim)\n        self.in_proj = nn.Linear(self.in_proj_in_dim, self.in_proj_out_dim, bias=False)\n        self.act_fn = nn.GELU()\n        self.out_proj = nn.Linear(\n            self.out_proj_in_dim, self.out_proj_out_dim, bias=False\n        )\n        self.gaussian_proj = nn.Linear(k, 1)\n        self.pos_embed_ff_norm = nn.LayerNorm(k)\n\n    def forward(\n        self,\n        x: torch.Tensor,\n        pos_embed: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass for the parallel block.\"\"\"\n        b, n, c = x.shape\n        res = x\n\n        # Combined MLP fc1 &amp; qkv projections\n        x = self.in_proj(self.in_norm(x))\n        x, q, k, v = torch.split(x, self.in_split, dim=-1)\n        x = self.act_fn(x)\n        x = self.proj_drop(x)\n\n        # Dot product attention\n        q = self.q_norm(q.view(b, n, self.num_heads, self.head_dim).transpose(1, 2))\n        k = self.k_norm(k.view(b, n, self.num_heads, self.head_dim).transpose(1, 2))\n        v = v.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n\n        x_attn = (\n            f.scaled_dot_product_attention(\n                q,\n                k,\n                v,\n                attn_mask=attention_mask\n                + self.gaussian_proj(self.pos_embed_ff_norm(pos_embed)).permute(\n                    0, 3, 1, 2\n                ),\n                is_causal=False,\n            )\n            .transpose(1, 2)\n            .reshape(b, n, c)\n        )\n\n        # Combined MLP fc2 &amp; attn_output projection\n        x_mlp, x_attn = self.out_proj(torch.cat([x, x_attn], dim=-1)).split(\n            self.out_split, dim=-1\n        )\n        # Residual connections\n        x = x_mlp + x_attn + res\n        del x_mlp, x_attn, res\n\n        return x, pos_embed\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.ParallelBlock.forward","title":"forward","text":"<pre><code>forward(x, pos_embed, attention_mask=None)\n</code></pre> <p>Forward pass for the parallel block.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    x: torch.Tensor,\n    pos_embed: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass for the parallel block.\"\"\"\n    b, n, c = x.shape\n    res = x\n\n    # Combined MLP fc1 &amp; qkv projections\n    x = self.in_proj(self.in_norm(x))\n    x, q, k, v = torch.split(x, self.in_split, dim=-1)\n    x = self.act_fn(x)\n    x = self.proj_drop(x)\n\n    # Dot product attention\n    q = self.q_norm(q.view(b, n, self.num_heads, self.head_dim).transpose(1, 2))\n    k = self.k_norm(k.view(b, n, self.num_heads, self.head_dim).transpose(1, 2))\n    v = v.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n\n    x_attn = (\n        f.scaled_dot_product_attention(\n            q,\n            k,\n            v,\n            attn_mask=attention_mask\n            + self.gaussian_proj(self.pos_embed_ff_norm(pos_embed)).permute(\n                0, 3, 1, 2\n            ),\n            is_causal=False,\n        )\n        .transpose(1, 2)\n        .reshape(b, n, c)\n    )\n\n    # Combined MLP fc2 &amp; attn_output projection\n    x_mlp, x_attn = self.out_proj(torch.cat([x, x_attn], dim=-1)).split(\n        self.out_split, dim=-1\n    )\n    # Residual connections\n    x = x_mlp + x_attn + res\n    del x_mlp, x_attn, res\n\n    return x, pos_embed\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomformerEncoder","title":"AtomformerEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Atomformer encoder.</p> <p>The transformer encoder consists of a series of parallel blocks, each containing a multi-head self-attention mechanism and a feed-forward network.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class AtomformerEncoder(nn.Module):\n    \"\"\"Atomformer encoder.\n\n    The transformer encoder consists of a series of parallel blocks,\n    each containing a multi-head self-attention mechanism and a feed-forward network.\n    \"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__()\n        self.vocab_size = config.vocab_size\n        self.dim = config.dim\n        self.num_heads = config.num_heads\n        self.depth = config.depth\n        self.mlp_ratio = config.mlp_ratio\n        self.dropout = config.dropout\n        self.k = config.k\n        self.gradient_checkpointing = config.gradient_checkpointing\n\n        self.metadata_vocab = nn.Embedding(self.vocab_size, 17)\n        self.metadata_vocab.weight.requires_grad = False\n        self.metadata_vocab.weight.fill_(-1)\n        self.metadata_vocab.weight[1:-4] = torch.tensor(\n            ATOM_METADATA, dtype=torch.float32\n        )\n        self.embed_metadata = nn.Linear(17, self.dim)\n\n        self.gaussian_embed = GaussianLayer(\n            k=self.k, edge_types=(self.vocab_size + 1) ** 2\n        )\n\n        self.token_type_embedding = nn.Embedding(4, self.dim)\n        nn.init.zeros_(self.token_type_embedding.weight)\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.dim)\n        nn.init.normal_(self.embed_tokens.weight, std=0.02)\n\n        self.blocks = nn.ModuleList()\n        for _ in range(self.depth):\n            self.blocks.append(\n                ParallelBlock(\n                    self.dim,\n                    self.num_heads,\n                    self.mlp_ratio,\n                    self.dropout,\n                    self.k,\n                    self.gradient_checkpointing,\n                )\n            )\n\n    def _expand_mask(\n        self,\n        mask: torch.Tensor,\n        dtype: torch.dtype,\n        device: torch.device,\n        tgt_len: Optional[int] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Expand attention mask.\n\n        Expands attention_mask from `[bsz, seq_len]` to\n        `[bsz, 1, tgt_seq_len, src_seq_len]`.\n        \"\"\"\n        bsz, src_len = mask.size()\n        tgt_len = tgt_len if tgt_len is not None else src_len\n\n        expanded_mask = (\n            mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n        )\n\n        inverted_mask: torch.Tensor = 1.0 - expanded_mask\n\n        return inverted_mask.masked_fill(\n            inverted_mask.to(torch.bool), torch.finfo(dtype).min\n        ).to(device)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n        \"\"\"Forward pass for the transformer encoder.\"\"\"\n        # pad coords by zeros for graph token\n        coords_center = torch.sum(coords, dim=1, keepdim=True) / coords.shape[1]\n        coords = torch.cat([coords_center, coords], dim=1)\n\n        r_ij = torch.cdist(coords, coords, p=2)  # [B, N, N]\n        # pad input_ids by graph token\n        input_ids = torch.cat(\n            [\n                torch.zeros(\n                    input_ids.size(0), 1, dtype=torch.long, device=input_ids.device\n                ).fill_(122),\n                input_ids,\n            ],\n            dim=1,\n        )\n        edge_type = input_ids.unsqueeze(-1) * self.vocab_size + input_ids.unsqueeze(\n            -2\n        )  # [B, N, N]\n        pos_embeds = self.gaussian_embed(r_ij, edge_type)  # [B, N, N, K]\n\n        input_embeds = self.embed_tokens(input_ids)\n        if token_type_ids is not None:\n            token_type_ids = torch.cat(\n                [\n                    torch.empty(\n                        input_ids.size(0), 1, dtype=torch.long, device=input_ids.device\n                    ).fill_(3),\n                    token_type_ids,\n                ],\n                dim=1,\n            )\n            token_type_embeddings = self.token_type_embedding(token_type_ids)\n            input_embeds = input_embeds + token_type_embeddings\n\n        atom_metadata = self.metadata_vocab(input_ids)\n        input_embeds = input_embeds + self.embed_metadata(atom_metadata)  # [B, N, C]\n\n        attention_mask = (\n            torch.cat(\n                [\n                    torch.ones(\n                        attention_mask.size(0),\n                        1,\n                        dtype=torch.bool,\n                        device=attention_mask.device,\n                    ),\n                    attention_mask.bool(),\n                ],\n                dim=1,\n            )\n            if attention_mask is not None\n            else None\n        )\n\n        attention_mask = (\n            self._expand_mask(attention_mask, input_embeds.dtype, input_embeds.device)\n            if attention_mask is not None\n            else None\n        )\n\n        for blk in self.blocks:\n            input_embeds, pos_embeds = blk(input_embeds, pos_embeds, attention_mask)\n\n        return input_embeds, pos_embeds\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomformerEncoder.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    attention_mask=None,\n    token_type_ids=None,\n)\n</code></pre> <p>Forward pass for the transformer encoder.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Forward pass for the transformer encoder.\"\"\"\n    # pad coords by zeros for graph token\n    coords_center = torch.sum(coords, dim=1, keepdim=True) / coords.shape[1]\n    coords = torch.cat([coords_center, coords], dim=1)\n\n    r_ij = torch.cdist(coords, coords, p=2)  # [B, N, N]\n    # pad input_ids by graph token\n    input_ids = torch.cat(\n        [\n            torch.zeros(\n                input_ids.size(0), 1, dtype=torch.long, device=input_ids.device\n            ).fill_(122),\n            input_ids,\n        ],\n        dim=1,\n    )\n    edge_type = input_ids.unsqueeze(-1) * self.vocab_size + input_ids.unsqueeze(\n        -2\n    )  # [B, N, N]\n    pos_embeds = self.gaussian_embed(r_ij, edge_type)  # [B, N, N, K]\n\n    input_embeds = self.embed_tokens(input_ids)\n    if token_type_ids is not None:\n        token_type_ids = torch.cat(\n            [\n                torch.empty(\n                    input_ids.size(0), 1, dtype=torch.long, device=input_ids.device\n                ).fill_(3),\n                token_type_ids,\n            ],\n            dim=1,\n        )\n        token_type_embeddings = self.token_type_embedding(token_type_ids)\n        input_embeds = input_embeds + token_type_embeddings\n\n    atom_metadata = self.metadata_vocab(input_ids)\n    input_embeds = input_embeds + self.embed_metadata(atom_metadata)  # [B, N, C]\n\n    attention_mask = (\n        torch.cat(\n            [\n                torch.ones(\n                    attention_mask.size(0),\n                    1,\n                    dtype=torch.bool,\n                    device=attention_mask.device,\n                ),\n                attention_mask.bool(),\n            ],\n            dim=1,\n        )\n        if attention_mask is not None\n        else None\n    )\n\n    attention_mask = (\n        self._expand_mask(attention_mask, input_embeds.dtype, input_embeds.device)\n        if attention_mask is not None\n        else None\n    )\n\n    for blk in self.blocks:\n        input_embeds, pos_embeds = blk(input_embeds, pos_embeds, attention_mask)\n\n    return input_embeds, pos_embeds\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomformerPreTrainedModel","title":"AtomformerPreTrainedModel","text":"<p>               Bases: <code>PreTrainedModel</code></p> <p>Base class for all transformer models.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class AtomformerPreTrainedModel(PreTrainedModel):  # type: ignore\n    \"\"\"Base class for all transformer models.\"\"\"\n\n    config_class = AtomformerConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"ParallelBlock\"]\n\n    def _set_gradient_checkpointing(\n        self, module: nn.Module, value: bool = False\n    ) -&gt; None:\n        if isinstance(module, (AtomformerEncoder)):\n            module.gradient_checkpointing = value\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomformerModel","title":"AtomformerModel","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer model for atom modeling.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class AtomformerModel(AtomformerPreTrainedModel):\n    \"\"\"Atomformer model for atom modeling.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = AtomformerEncoder(config)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward function call for the transformer model.\"\"\"\n        output: torch.Tensor = self.encoder(input_ids, coords, attention_mask)\n        return output\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomformerModel.forward","title":"forward","text":"<pre><code>forward(input_ids, coords, attention_mask=None)\n</code></pre> <p>Forward function call for the transformer model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Forward function call for the transformer model.\"\"\"\n    output: torch.Tensor = self.encoder(input_ids, coords, attention_mask)\n    return output\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomformerForMaskedAM","title":"AtomformerForMaskedAM","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer with an atom modeling head on top for masked atom modeling.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class AtomformerForMaskedAM(AtomformerPreTrainedModel):\n    \"\"\"Atomformer with an atom modeling head on top for masked atom modeling.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = AtomformerEncoder(config)\n        self.am_head = nn.Linear(config.dim, config.vocab_size, bias=False)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels: Optional[torch.Tensor] = None,\n        fixed: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n        \"\"\"Forward function call for the masked atom modeling model.\"\"\"\n        hidden_states = self.encoder(input_ids, coords, attention_mask)\n        logits = self.am_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            logits, labels = logits.view(-1, self.config.vocab_size), labels.view(-1)\n            loss = loss_fct(logits, labels)\n\n        return loss, logits\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomformerForMaskedAM.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels=None,\n    fixed=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the masked atom modeling model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels: Optional[torch.Tensor] = None,\n    fixed: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"Forward function call for the masked atom modeling model.\"\"\"\n    hidden_states = self.encoder(input_ids, coords, attention_mask)\n    logits = self.am_head(hidden_states)\n\n    loss = None\n    if labels is not None:\n        loss_fct = nn.CrossEntropyLoss()\n        logits, labels = logits.view(-1, self.config.vocab_size), labels.view(-1)\n        loss = loss_fct(logits, labels)\n\n    return loss, logits\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomformerForCoordinateAM","title":"AtomformerForCoordinateAM","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer with an atom coordinate head on top for coordinate denoising.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class AtomformerForCoordinateAM(AtomformerPreTrainedModel):\n    \"\"\"Atomformer with an atom coordinate head on top for coordinate denoising.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = AtomformerEncoder(config)\n        self.coords_head = nn.Linear(config.dim, 3)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels_coords: Optional[torch.Tensor] = None,\n        fixed: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n        \"\"\"Forward function call for the coordinate atom modeling model.\"\"\"\n        hidden_states = self.encoder(input_ids, coords, attention_mask)\n        coords_pred = self.coords_head(hidden_states)\n\n        loss = None\n        if labels_coords is not None:\n            labels_coords = labels_coords.to(coords_pred.device)\n            loss_fct = nn.L1Loss()\n            loss = loss_fct(coords_pred, labels_coords)\n\n        return loss, coords_pred\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomformerForCoordinateAM.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels_coords=None,\n    fixed=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the coordinate atom modeling model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels_coords: Optional[torch.Tensor] = None,\n    fixed: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"Forward function call for the coordinate atom modeling model.\"\"\"\n    hidden_states = self.encoder(input_ids, coords, attention_mask)\n    coords_pred = self.coords_head(hidden_states)\n\n    loss = None\n    if labels_coords is not None:\n        labels_coords = labels_coords.to(coords_pred.device)\n        loss_fct = nn.L1Loss()\n        loss = loss_fct(coords_pred, labels_coords)\n\n    return loss, coords_pred\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.InitialStructure2RelaxedStructure","title":"InitialStructure2RelaxedStructure","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer with an coordinate head on top for relaxed structure prediction.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class InitialStructure2RelaxedStructure(AtomformerPreTrainedModel):\n    \"\"\"Atomformer with an coordinate head on top for relaxed structure prediction.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = AtomformerEncoder(config)\n        self.coords_head = nn.Linear(config.dim, 3)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels_coords: Optional[torch.Tensor] = None,\n        fixed: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n        \"\"\"Forward function call.\n\n        Initial structure to relaxed structure model.\n        \"\"\"\n        hidden_states = self.encoder(input_ids, coords, attention_mask)\n        coords_pred = self.coords_head(hidden_states)\n\n        loss = None\n        if labels_coords is not None:\n            labels_coords = labels_coords.to(coords_pred.device)\n            loss_fct = nn.L1Loss()\n            loss = loss_fct(coords_pred, labels_coords)\n\n        return loss, coords_pred\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.InitialStructure2RelaxedStructure.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels_coords=None,\n    fixed=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call.</p> <p>Initial structure to relaxed structure model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels_coords: Optional[torch.Tensor] = None,\n    fixed: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"Forward function call.\n\n    Initial structure to relaxed structure model.\n    \"\"\"\n    hidden_states = self.encoder(input_ids, coords, attention_mask)\n    coords_pred = self.coords_head(hidden_states)\n\n    loss = None\n    if labels_coords is not None:\n        labels_coords = labels_coords.to(coords_pred.device)\n        loss_fct = nn.L1Loss()\n        loss = loss_fct(coords_pred, labels_coords)\n\n    return loss, coords_pred\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.InitialStructure2RelaxedEnergy","title":"InitialStructure2RelaxedEnergy","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer with an energy head on top for relaxed energy prediction.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class InitialStructure2RelaxedEnergy(AtomformerPreTrainedModel):\n    \"\"\"Atomformer with an energy head on top for relaxed energy prediction.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = AtomformerEncoder(config)\n        self.energy_norm = nn.LayerNorm(config.dim)\n        self.energy_head = nn.Linear(config.dim, 1, bias=False)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels_energy: Optional[torch.Tensor] = None,\n        fixed: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n        \"\"\"Forward function call for the relaxed energy prediction model.\"\"\"\n        hidden_states = self.encoder(input_ids, coords, attention_mask)\n        energy = self.energy_head(self.energy_norm(hidden_states[:, 0])).squeeze(-1)\n\n        loss = None\n        if labels_energy is not None:\n            loss_fct = nn.L1Loss()\n            loss = loss_fct(energy, labels_energy)\n\n        return loss, energy\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.InitialStructure2RelaxedEnergy.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels_energy=None,\n    fixed=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the relaxed energy prediction model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels_energy: Optional[torch.Tensor] = None,\n    fixed: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"Forward function call for the relaxed energy prediction model.\"\"\"\n    hidden_states = self.encoder(input_ids, coords, attention_mask)\n    energy = self.energy_head(self.energy_norm(hidden_states[:, 0])).squeeze(-1)\n\n    loss = None\n    if labels_energy is not None:\n        loss_fct = nn.L1Loss()\n        loss = loss_fct(energy, labels_energy)\n\n    return loss, energy\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.InitialStructure2RelaxedStructureAndEnergy","title":"InitialStructure2RelaxedStructureAndEnergy","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer with an coordinate and energy head.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class InitialStructure2RelaxedStructureAndEnergy(AtomformerPreTrainedModel):\n    \"\"\"Atomformer with an coordinate and energy head.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = AtomformerEncoder(config)\n        self.energy_norm = nn.LayerNorm(config.dim)\n        self.energy_head = nn.Linear(config.dim, 1, bias=False)\n        self.coords_head = nn.Linear(config.dim, 3)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels_coords: Optional[torch.Tensor] = None,\n        forces: Optional[torch.Tensor] = None,\n        total_energy: Optional[torch.Tensor] = None,\n        formation_energy: Optional[torch.Tensor] = None,\n        has_formation_energy: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"Forward function call for the relaxed structure and energy model.\"\"\"\n        atom_hidden_states, pos_hidden_states = self.encoder(\n            input_ids, coords, attention_mask\n        )\n\n        formation_energy_pred = self.formation_energy_head(\n            self.energy_norm(atom_hidden_states[:, 0])\n        ).squeeze(-1)\n        loss_formation_energy = None\n        if formation_energy is not None:\n            loss_fct = nn.L1Loss()\n            loss_formation_energy = loss_fct(\n                formation_energy_pred[has_formation_energy],\n                formation_energy[has_formation_energy],\n            )\n        coords_pred = self.coords_head(atom_hidden_states[:, 1:])\n        loss_coords = None\n        if labels_coords is not None:\n            loss_fct = nn.L1Loss()\n            loss_coords = loss_fct(coords_pred, labels_coords)\n\n        loss = torch.Tensor(0).to(coords.device)\n        loss = (\n            loss + loss_formation_energy if loss_formation_energy is not None else loss\n        )\n        loss = loss + loss_coords if loss_coords is not None else loss\n\n        return loss, (formation_energy_pred, coords_pred)\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.InitialStructure2RelaxedStructureAndEnergy.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels_coords=None,\n    forces=None,\n    total_energy=None,\n    formation_energy=None,\n    has_formation_energy=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the relaxed structure and energy model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels_coords: Optional[torch.Tensor] = None,\n    forces: Optional[torch.Tensor] = None,\n    total_energy: Optional[torch.Tensor] = None,\n    formation_energy: Optional[torch.Tensor] = None,\n    has_formation_energy: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"Forward function call for the relaxed structure and energy model.\"\"\"\n    atom_hidden_states, pos_hidden_states = self.encoder(\n        input_ids, coords, attention_mask\n    )\n\n    formation_energy_pred = self.formation_energy_head(\n        self.energy_norm(atom_hidden_states[:, 0])\n    ).squeeze(-1)\n    loss_formation_energy = None\n    if formation_energy is not None:\n        loss_fct = nn.L1Loss()\n        loss_formation_energy = loss_fct(\n            formation_energy_pred[has_formation_energy],\n            formation_energy[has_formation_energy],\n        )\n    coords_pred = self.coords_head(atom_hidden_states[:, 1:])\n    loss_coords = None\n    if labels_coords is not None:\n        loss_fct = nn.L1Loss()\n        loss_coords = loss_fct(coords_pred, labels_coords)\n\n    loss = torch.Tensor(0).to(coords.device)\n    loss = (\n        loss + loss_formation_energy if loss_formation_energy is not None else loss\n    )\n    loss = loss + loss_coords if loss_coords is not None else loss\n\n    return loss, (formation_energy_pred, coords_pred)\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.Structure2Energy","title":"Structure2Energy","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer with an atom modeling head on top for masked atom modeling.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class Structure2Energy(AtomformerPreTrainedModel):\n    \"\"\"Atomformer with an atom modeling head on top for masked atom modeling.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = AtomformerEncoder(config)\n        self.energy_norm = nn.LayerNorm(config.dim)\n        self.formation_energy_head = nn.Linear(config.dim, 1, bias=False)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        forces: Optional[torch.Tensor] = None,\n        total_energy: Optional[torch.Tensor] = None,\n        formation_energy: Optional[torch.Tensor] = None,\n        has_formation_energy: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], Tuple[torch.Tensor, Optional[torch.Tensor]]]:\n        \"\"\"Forward function call for the structure to energy model.\"\"\"\n        atom_hidden_states, pos_hidden_states = self.encoder(\n            input_ids, coords, attention_mask\n        )\n\n        formation_energy_pred: torch.Tensor = self.formation_energy_head(\n            self.energy_norm(atom_hidden_states[:, 0])\n        ).squeeze(-1)\n        loss = torch.Tensor(0).to(coords.device)\n        if formation_energy is not None:\n            loss_fct = nn.L1Loss()\n            loss = loss_fct(\n                formation_energy_pred[has_formation_energy],\n                formation_energy[has_formation_energy],\n            )\n\n        return loss, (\n            formation_energy_pred,\n            attention_mask.bool() if attention_mask is not None else None,\n        )\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.Structure2Energy.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    forces=None,\n    total_energy=None,\n    formation_energy=None,\n    has_formation_energy=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the structure to energy model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    forces: Optional[torch.Tensor] = None,\n    total_energy: Optional[torch.Tensor] = None,\n    formation_energy: Optional[torch.Tensor] = None,\n    has_formation_energy: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], Tuple[torch.Tensor, Optional[torch.Tensor]]]:\n    \"\"\"Forward function call for the structure to energy model.\"\"\"\n    atom_hidden_states, pos_hidden_states = self.encoder(\n        input_ids, coords, attention_mask\n    )\n\n    formation_energy_pred: torch.Tensor = self.formation_energy_head(\n        self.energy_norm(atom_hidden_states[:, 0])\n    ).squeeze(-1)\n    loss = torch.Tensor(0).to(coords.device)\n    if formation_energy is not None:\n        loss_fct = nn.L1Loss()\n        loss = loss_fct(\n            formation_energy_pred[has_formation_energy],\n            formation_energy[has_formation_energy],\n        )\n\n    return loss, (\n        formation_energy_pred,\n        attention_mask.bool() if attention_mask is not None else None,\n    )\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.Structure2Forces","title":"Structure2Forces","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer with a forces head on top for forces prediction.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class Structure2Forces(AtomformerPreTrainedModel):\n    \"\"\"Atomformer with a forces head on top for forces prediction.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = AtomformerEncoder(config)\n        self.force_norm = nn.LayerNorm(config.dim)\n        self.force_head = nn.Linear(config.dim, 3)\n        self.energy_norm = nn.LayerNorm(config.dim)\n        self.formation_energy_head = nn.Linear(config.dim, 1, bias=False)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        forces: Optional[torch.Tensor] = None,\n        total_energy: Optional[torch.Tensor] = None,\n        formation_energy: Optional[torch.Tensor] = None,\n        has_formation_energy: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:\n        \"\"\"Forward function call for the structure to forces model.\"\"\"\n        atom_hidden_states, pos_hidden_states = self.encoder(\n            input_ids, coords, attention_mask\n        )\n        attention_mask = attention_mask.bool() if attention_mask is not None else None\n\n        forces_pred: torch.Tensor = self.force_head(\n            self.force_norm(atom_hidden_states[:, 1:])\n        )\n        loss = torch.Tensor(0).to(coords.device)\n        if forces is not None:\n            loss_fct = nn.L1Loss()\n            loss = loss_fct(forces_pred[attention_mask], forces[attention_mask])\n\n        return loss, (\n            forces_pred,\n            attention_mask if attention_mask is not None else None,\n        )\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.Structure2Forces.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    forces=None,\n    total_energy=None,\n    formation_energy=None,\n    has_formation_energy=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the structure to forces model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    forces: Optional[torch.Tensor] = None,\n    total_energy: Optional[torch.Tensor] = None,\n    formation_energy: Optional[torch.Tensor] = None,\n    has_formation_energy: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, Optional[torch.Tensor]]]:\n    \"\"\"Forward function call for the structure to forces model.\"\"\"\n    atom_hidden_states, pos_hidden_states = self.encoder(\n        input_ids, coords, attention_mask\n    )\n    attention_mask = attention_mask.bool() if attention_mask is not None else None\n\n    forces_pred: torch.Tensor = self.force_head(\n        self.force_norm(atom_hidden_states[:, 1:])\n    )\n    loss = torch.Tensor(0).to(coords.device)\n    if forces is not None:\n        loss_fct = nn.L1Loss()\n        loss = loss_fct(forces_pred[attention_mask], forces[attention_mask])\n\n    return loss, (\n        forces_pred,\n        attention_mask if attention_mask is not None else None,\n    )\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.Structure2EnergyAndForces","title":"Structure2EnergyAndForces","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer with an energy and forces head for energy and forces prediction.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class Structure2EnergyAndForces(AtomformerPreTrainedModel):\n    \"\"\"Atomformer with an energy and forces head for energy and forces prediction.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = AtomformerEncoder(config)\n        self.force_norm = nn.LayerNorm(config.dim)\n        self.force_head = nn.Linear(config.dim, 3)\n        self.energy_norm = nn.LayerNorm(config.dim)\n        self.formation_energy_head = nn.Linear(config.dim, 1, bias=False)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        forces: Optional[torch.Tensor] = None,\n        total_energy: Optional[torch.Tensor] = None,\n        formation_energy: Optional[torch.Tensor] = None,\n        has_formation_energy: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]]:\n        \"\"\"Forward function call for the structure to energy and forces model.\"\"\"\n        atom_hidden_states, pos_hidden_states = self.encoder(\n            input_ids, coords, attention_mask\n        )\n\n        formation_energy_pred: torch.Tensor = self.formation_energy_head(\n            self.energy_norm(atom_hidden_states[:, 0])\n        ).squeeze(-1)\n        loss_formation_energy = None\n        if formation_energy is not None:\n            loss_fct = nn.L1Loss()\n            loss_formation_energy = loss_fct(\n                formation_energy_pred[has_formation_energy],\n                formation_energy[has_formation_energy],\n            )\n            loss = loss_formation_energy\n        attention_mask = attention_mask.bool() if attention_mask is not None else None\n        forces_pred: torch.Tensor = self.force_head(\n            self.force_norm(atom_hidden_states[:, 1:])\n        )\n        loss_forces = None\n        if forces is not None:\n            loss_fct = nn.L1Loss()\n            loss_forces = loss_fct(forces_pred[attention_mask], forces[attention_mask])\n            loss = loss + loss_forces if loss is not None else loss_forces\n\n        return loss, (formation_energy_pred, forces_pred, attention_mask)\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.Structure2EnergyAndForces.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    forces=None,\n    total_energy=None,\n    formation_energy=None,\n    has_formation_energy=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the structure to energy and forces model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    forces: Optional[torch.Tensor] = None,\n    total_energy: Optional[torch.Tensor] = None,\n    formation_energy: Optional[torch.Tensor] = None,\n    has_formation_energy: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]]:\n    \"\"\"Forward function call for the structure to energy and forces model.\"\"\"\n    atom_hidden_states, pos_hidden_states = self.encoder(\n        input_ids, coords, attention_mask\n    )\n\n    formation_energy_pred: torch.Tensor = self.formation_energy_head(\n        self.energy_norm(atom_hidden_states[:, 0])\n    ).squeeze(-1)\n    loss_formation_energy = None\n    if formation_energy is not None:\n        loss_fct = nn.L1Loss()\n        loss_formation_energy = loss_fct(\n            formation_energy_pred[has_formation_energy],\n            formation_energy[has_formation_energy],\n        )\n        loss = loss_formation_energy\n    attention_mask = attention_mask.bool() if attention_mask is not None else None\n    forces_pred: torch.Tensor = self.force_head(\n        self.force_norm(atom_hidden_states[:, 1:])\n    )\n    loss_forces = None\n    if forces is not None:\n        loss_fct = nn.L1Loss()\n        loss_forces = loss_fct(forces_pred[attention_mask], forces[attention_mask])\n        loss = loss + loss_forces if loss is not None else loss_forces\n\n    return loss, (formation_energy_pred, forces_pred, attention_mask)\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.Structure2TotalEnergyAndForces","title":"Structure2TotalEnergyAndForces","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer with an energy and forces head for energy and forces prediction.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class Structure2TotalEnergyAndForces(AtomformerPreTrainedModel):\n    \"\"\"Atomformer with an energy and forces head for energy and forces prediction.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = AtomformerEncoder(config)\n        self.force_norm = nn.LayerNorm(config.dim)\n        self.force_head = nn.Linear(config.dim, 3, bias=False)\n        nn.init.zeros_(self.force_head.weight)\n        self.energy_norm = nn.LayerNorm(config.dim)\n        self.total_energy_head = nn.Linear(config.dim, 1, bias=False)\n        nn.init.zeros_(self.total_energy_head.weight)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        forces: Optional[torch.Tensor] = None,\n        total_energy: Optional[torch.Tensor] = None,\n        formation_energy: Optional[torch.Tensor] = None,\n        has_formation_energy: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[\n        Optional[torch.Tensor],\n        Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]],\n    ]:\n        \"\"\"Forward function call for the structure to energy and forces model.\"\"\"\n        atom_hidden_states, pos_hidden_states = self.encoder(\n            input_ids, coords, attention_mask\n        )\n\n        loss = None\n        total_energy_pred: torch.Tensor = self.total_energy_head(\n            self.energy_norm(atom_hidden_states[:, 0])\n        ).squeeze(-1)\n        loss_total_energy = None\n        if formation_energy is not None:\n            loss_fct = nn.L1Loss()\n            loss_total_energy = loss_fct(\n                total_energy_pred,\n                total_energy,\n            )\n            loss = loss_total_energy\n        attention_mask = attention_mask.bool() if attention_mask is not None else None\n        forces_pred: torch.Tensor = self.force_head(\n            self.force_norm(atom_hidden_states[:, 1:])\n        )\n        loss_forces = None\n        if forces is not None:\n            loss_fct = nn.L1Loss()\n            loss_forces = loss_fct(forces_pred[attention_mask], forces[attention_mask])\n            loss = loss + loss_forces if loss is not None else loss_forces\n\n        return loss, (total_energy_pred, forces_pred, attention_mask)\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.Structure2TotalEnergyAndForces.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    forces=None,\n    total_energy=None,\n    formation_energy=None,\n    has_formation_energy=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the structure to energy and forces model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    forces: Optional[torch.Tensor] = None,\n    total_energy: Optional[torch.Tensor] = None,\n    formation_energy: Optional[torch.Tensor] = None,\n    has_formation_energy: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[\n    Optional[torch.Tensor],\n    Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]],\n]:\n    \"\"\"Forward function call for the structure to energy and forces model.\"\"\"\n    atom_hidden_states, pos_hidden_states = self.encoder(\n        input_ids, coords, attention_mask\n    )\n\n    loss = None\n    total_energy_pred: torch.Tensor = self.total_energy_head(\n        self.energy_norm(atom_hidden_states[:, 0])\n    ).squeeze(-1)\n    loss_total_energy = None\n    if formation_energy is not None:\n        loss_fct = nn.L1Loss()\n        loss_total_energy = loss_fct(\n            total_energy_pred,\n            total_energy,\n        )\n        loss = loss_total_energy\n    attention_mask = attention_mask.bool() if attention_mask is not None else None\n    forces_pred: torch.Tensor = self.force_head(\n        self.force_norm(atom_hidden_states[:, 1:])\n    )\n    loss_forces = None\n    if forces is not None:\n        loss_fct = nn.L1Loss()\n        loss_forces = loss_fct(forces_pred[attention_mask], forces[attention_mask])\n        loss = loss + loss_forces if loss is not None else loss_forces\n\n    return loss, (total_energy_pred, forces_pred, attention_mask)\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomFormerForSystemClassification","title":"AtomFormerForSystemClassification","text":"<p>               Bases: <code>AtomformerPreTrainedModel</code></p> <p>Atomformer with a classification head for system classification.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>class AtomFormerForSystemClassification(AtomformerPreTrainedModel):\n    \"\"\"Atomformer with a classification head for system classification.\"\"\"\n\n    def __init__(self, config: AtomformerConfig):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.problem_type = config.problem_type\n        self.config = config\n\n        self.encoder = AtomformerEncoder(config)\n\n        self.cls_norm = nn.LayerNorm(config.dim)\n        self.classification_head = nn.Linear(config.dim, self.num_labels, bias=False)\n        nn.init.zeros_(self.classification_head.weight)\n\n        self.loss_fct: Union[nn.L1Loss, nn.BCEWithLogitsLoss, nn.CrossEntropyLoss]\n\n        if self.problem_type == \"regression\":\n            self.loss_fct = nn.L1Loss()\n        elif self.problem_type == \"classification\":\n            self.loss_fct = nn.BCEWithLogitsLoss()\n        elif self.problem_type == \"multiclass_classification\":\n            self.loss_fct = nn.CrossEntropyLoss()\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n        \"\"\"Forward function call for the structure to energy and forces model.\"\"\"\n        atom_hidden_states, pos_hidden_states = self.encoder(\n            input_ids, coords, attention_mask, token_type_ids\n        )\n        pred = self.classification_head(self.cls_norm(atom_hidden_states[:, 0]))\n\n        loss = None\n        if labels is not None:\n            if self.problem_type == \"multiclass_classification\":\n                labels = labels.long()\n            elif self.problem_type == \"classification\":\n                labels = labels.float()\n\n            loss = self.loss_fct(pred.squeeze(), labels.squeeze())\n\n        return loss, pred\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.AtomFormerForSystemClassification.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels=None,\n    attention_mask=None,\n    token_type_ids=None,\n)\n</code></pre> <p>Forward function call for the structure to energy and forces model.</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    token_type_ids: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"Forward function call for the structure to energy and forces model.\"\"\"\n    atom_hidden_states, pos_hidden_states = self.encoder(\n        input_ids, coords, attention_mask, token_type_ids\n    )\n    pred = self.classification_head(self.cls_norm(atom_hidden_states[:, 0]))\n\n    loss = None\n    if labels is not None:\n        if self.problem_type == \"multiclass_classification\":\n            labels = labels.long()\n        elif self.problem_type == \"classification\":\n            labels = labels.float()\n\n        loss = self.loss_fct(pred.squeeze(), labels.squeeze())\n\n    return loss, pred\n</code></pre>"},{"location":"api/#atomgen.models.modeling_atomformer.gaussian","title":"gaussian","text":"<pre><code>gaussian(x, mean, std)\n</code></pre> <p>Compute the Gaussian distribution probability density.</p> <p>Taken from: https://https://github.com/microsoft/Graphormer/blob/main/graphormer/models/graphormer_3d.py</p> Source code in <code>atomgen/models/modeling_atomformer.py</code> <pre><code>@torch.jit.script\ndef gaussian(x: torch.Tensor, mean: torch.Tensor, std: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute the Gaussian distribution probability density.\n\n    Taken from: https://https://github.com/microsoft/Graphormer/blob/main/graphormer/models/graphormer_3d.py\n\n    \"\"\"\n    pi = 3.14159\n    a = (2 * pi) ** 0.5\n    output: torch.Tensor = torch.exp(-0.5 * (((x - mean) / std) ** 2)) / (a * std)\n    return output\n</code></pre>"},{"location":"api/#atomgen.models.schnet","title":"schnet","text":"<p>SchNet model for energy prediction.</p>"},{"location":"api/#atomgen.models.schnet.SchNetConfig","title":"SchNetConfig","text":"<p>               Bases: <code>PretrainedConfig</code></p> <p>Stores the configuration of a :class:<code>~transformers.SchNetModel</code>.</p> <p>It is used to instantiate an SchNet model according to the specified arguments, defining the model architecture.</p> <p>Args:     vocab_size (:obj:<code>int</code>, <code>optional</code>, defaults to 122):         The size of the vocabulary, used to define the size         of the output embeddings.</p> <pre><code>hidden_channels (:obj:`int`, `optional`, defaults to 128):\n    The hidden size of the model.\n</code></pre> <p>model_type = \"transformer\"</p> <p>Attributes:</p> Name Type Description <code>vocab_size (</code> <code>obj:`int`):</code> <pre><code>The size of the vocabulary, used to define\nthe size of the output embeddings.\n</code></pre> <p>hidden_channels (:obj:<code>int</code>):     The hidden size of the model.</p> <p>num_filters (:obj:<code>int</code>):     The number of filters.</p> <p>num_interactions (:obj:<code>int</code>):     The number of interactions.</p> <p>num_gaussians (:obj:<code>int</code>):     The number of gaussians.</p> <p>cutoff (:obj:<code>float</code>):     The cutoff value.</p> <p>interaction_graph (:obj:<code>str</code>, <code>optional</code>):     The interaction graph.</p> <p>max_num_neighbors (:obj:<code>int</code>):     The maximum number of neighbors.</p> <p>readout (:obj:<code>str</code>, <code>optional</code>):     The readout method.</p> <p>dipole (:obj:<code>bool</code>, <code>optional</code>):     Whether to include dipole.</p> <p>mean (:obj:<code>float</code>, <code>optional</code>):     The mean value.</p> <p>std (:obj:<code>float</code>, <code>optional</code>):     The standard deviation value.</p> <p>atomref (:obj:<code>float</code>, <code>optional</code>):     The atom reference value.</p> <p>mask_token_id (:obj:<code>int</code>, <code>optional</code>):     The token ID for masking.</p> <p>pad_token_id (:obj:<code>int</code>, <code>optional</code>):     The token ID for padding.</p> <p>bos_token_id (:obj:<code>int</code>, <code>optional</code>):     The token ID for the beginning of sequence.</p> <p>eos_token_id (:obj:<code>int</code>, <code>optional</code>):     The token ID for the end of sequence.</p> Source code in <code>atomgen/models/schnet.py</code> <pre><code>class SchNetConfig(PretrainedConfig):  # type: ignore\n    r\"\"\"\n    Stores the configuration of a :class:`~transformers.SchNetModel`.\n\n    It is used to instantiate an SchNet model according to the specified arguments,\n    defining the model architecture.\n\n    Args:\n        vocab_size (:obj:`int`, `optional`, defaults to 122):\n            The size of the vocabulary, used to define the size\n            of the output embeddings.\n\n        hidden_channels (:obj:`int`, `optional`, defaults to 128):\n            The hidden size of the model.\n\n    model_type = \"transformer\"\n\n    Attributes\n    ----------\n        vocab_size (:obj:`int`):\n            The size of the vocabulary, used to define\n            the size of the output embeddings.\n\n        hidden_channels (:obj:`int`):\n            The hidden size of the model.\n\n        num_filters (:obj:`int`):\n            The number of filters.\n\n        num_interactions (:obj:`int`):\n            The number of interactions.\n\n        num_gaussians (:obj:`int`):\n            The number of gaussians.\n\n        cutoff (:obj:`float`):\n            The cutoff value.\n\n        interaction_graph (:obj:`str`, `optional`):\n            The interaction graph.\n\n        max_num_neighbors (:obj:`int`):\n            The maximum number of neighbors.\n\n        readout (:obj:`str`, `optional`):\n            The readout method.\n\n        dipole (:obj:`bool`, `optional`):\n            Whether to include dipole.\n\n        mean (:obj:`float`, `optional`):\n            The mean value.\n\n        std (:obj:`float`, `optional`):\n            The standard deviation value.\n\n        atomref (:obj:`float`, `optional`):\n            The atom reference value.\n\n        mask_token_id (:obj:`int`, `optional`):\n            The token ID for masking.\n\n        pad_token_id (:obj:`int`, `optional`):\n            The token ID for padding.\n\n        bos_token_id (:obj:`int`, `optional`):\n            The token ID for the beginning of sequence.\n\n        eos_token_id (:obj:`int`, `optional`):\n            The token ID for the end of sequence.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int = 123,\n        hidden_channels: int = 128,\n        num_filters: int = 128,\n        num_interactions: int = 6,\n        num_gaussians: int = 50,\n        cutoff: float = 10.0,\n        interaction_graph: Optional[Callable[..., Any]] = None,\n        max_num_neighbors: int = 32,\n        readout: str = \"add\",\n        dipole: bool = False,\n        mean: Optional[float] = None,\n        std: Optional[float] = None,\n        atomref: Optional[OptTensor] = None,\n        mask_token_id: int = 0,\n        pad_token_id: int = 119,\n        bos_token_id: int = 120,\n        eos_token_id: int = 121,\n        cls_token_id: int = 122,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)\n        self.vocab_size = vocab_size\n        self.hidden_channels = hidden_channels\n        self.num_filters = num_filters\n        self.num_interactions = num_interactions\n        self.num_gaussians = num_gaussians\n        self.cutoff = cutoff\n        self.interaction_graph = interaction_graph\n        self.max_num_neighbors = max_num_neighbors\n        self.readout = readout\n        self.dipole = dipole\n        self.mean = mean\n        self.std = std\n        self.atomref = atomref\n        self.mask_token_id = mask_token_id\n        self.pad_token_id = pad_token_id\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        self.cls_token_id = cls_token_id\n</code></pre>"},{"location":"api/#atomgen.models.schnet.SchNetPreTrainedModel","title":"SchNetPreTrainedModel","text":"<p>               Bases: <code>PreTrainedModel</code></p> <p>A base class for all SchNet models.</p> <p>An abstract class to handle weights initialization and a simple interface for loading and exporting models.</p> Source code in <code>atomgen/models/schnet.py</code> <pre><code>class SchNetPreTrainedModel(PreTrainedModel):  # type: ignore\n    \"\"\"\n    A base class for all SchNet models.\n\n    An abstract class to handle weights initialization and a\n    simple interface for loading and exporting models.\n    \"\"\"\n\n    config_class = SchNetConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = False\n</code></pre>"},{"location":"api/#atomgen.models.schnet.SchNetModel","title":"SchNetModel","text":"<p>               Bases: <code>SchNetPreTrainedModel</code></p> <p>SchNet model for energy prediction.</p> <p>Args:     config (:class:<code>~transformers.SchNetConfig</code>):         Configuration class to store the configuration of a model.</p> Source code in <code>atomgen/models/schnet.py</code> <pre><code>class SchNetModel(SchNetPreTrainedModel):\n    \"\"\"\n    SchNet model for energy prediction.\n\n    Args:\n        config (:class:`~transformers.SchNetConfig`):\n            Configuration class to store the configuration of a model.\n    \"\"\"\n\n    def __init__(self, config: SchNetConfig):\n        super().__init__(config)\n        self.config = config\n        self.model = SchNet(\n            hidden_channels=config.hidden_channels,\n            num_filters=config.num_filters,\n            num_interactions=config.num_interactions,\n            num_gaussians=config.num_gaussians,\n            cutoff=config.cutoff,\n            interaction_graph=config.interaction_graph,\n            max_num_neighbors=config.max_num_neighbors,\n            readout=config.readout,\n            dipole=config.dipole,\n            mean=config.mean,\n            std=config.std,\n            atomref=config.atomref,\n        )\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        batch: torch.Tensor,\n        labels_energy: Optional[torch.Tensor] = None,\n        fixed: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n        \"\"\"\n        Forward pass of the SchNet model.\n\n        Args:\n            input_ids (:obj:`torch.Tensor` of shape :obj:`(batch_size, num_atoms)`):\n                The input tensor containing the atom indices.\n\n            coords (:obj:`torch.Tensor` of shape :obj:`(num_atoms, 3)`):\n                The input tensor containing the atom coordinates.\n\n            batch (:obj:`torch.Tensor` of shape :obj:`(num_atoms)`):\n                The input tensor containing the batch indices.\n\n            labels_energy (:obj:`torch.Tensor`, `optional`):\n                The input tensor containing the energy labels.\n\n            fixed (:obj:`torch.Tensor`, `optional`):\n                The input tensor containing the fixed mask.\n\n            attention_mask (:obj:`torch.Tensor`, `optional`):\n                The attention mask for the transformer.\n\n        Returns\n        -------\n            :obj:`tuple`:\n                A tuple of the loss and the energy prediction.\n        \"\"\"\n        energy_pred: torch.Tensor = self.model(z=input_ids, pos=coords, batch=batch)\n\n        loss = None\n        if labels_energy is not None:\n            labels_energy = labels_energy.to(energy_pred.device)\n            loss_fct = nn.L1Loss()\n            loss = loss_fct(energy_pred.squeeze(-1), labels_energy)\n        return loss, energy_pred\n</code></pre>"},{"location":"api/#atomgen.models.schnet.SchNetModel.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    batch,\n    labels_energy=None,\n    fixed=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward pass of the SchNet model.</p> <p>Args:     input_ids (:obj:<code>torch.Tensor</code> of shape :obj:<code>(batch_size, num_atoms)</code>):         The input tensor containing the atom indices.</p> <pre><code>coords (:obj:`torch.Tensor` of shape :obj:`(num_atoms, 3)`):\n    The input tensor containing the atom coordinates.\n\nbatch (:obj:`torch.Tensor` of shape :obj:`(num_atoms)`):\n    The input tensor containing the batch indices.\n\nlabels_energy (:obj:`torch.Tensor`, `optional`):\n    The input tensor containing the energy labels.\n\nfixed (:obj:`torch.Tensor`, `optional`):\n    The input tensor containing the fixed mask.\n\nattention_mask (:obj:`torch.Tensor`, `optional`):\n    The attention mask for the transformer.\n</code></pre> <p>Returns:</p> Type Description <code>    :obj:`tuple`:</code> <p>A tuple of the loss and the energy prediction.</p> Source code in <code>atomgen/models/schnet.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    batch: torch.Tensor,\n    labels_energy: Optional[torch.Tensor] = None,\n    fixed: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"\n    Forward pass of the SchNet model.\n\n    Args:\n        input_ids (:obj:`torch.Tensor` of shape :obj:`(batch_size, num_atoms)`):\n            The input tensor containing the atom indices.\n\n        coords (:obj:`torch.Tensor` of shape :obj:`(num_atoms, 3)`):\n            The input tensor containing the atom coordinates.\n\n        batch (:obj:`torch.Tensor` of shape :obj:`(num_atoms)`):\n            The input tensor containing the batch indices.\n\n        labels_energy (:obj:`torch.Tensor`, `optional`):\n            The input tensor containing the energy labels.\n\n        fixed (:obj:`torch.Tensor`, `optional`):\n            The input tensor containing the fixed mask.\n\n        attention_mask (:obj:`torch.Tensor`, `optional`):\n            The attention mask for the transformer.\n\n    Returns\n    -------\n        :obj:`tuple`:\n            A tuple of the loss and the energy prediction.\n    \"\"\"\n    energy_pred: torch.Tensor = self.model(z=input_ids, pos=coords, batch=batch)\n\n    loss = None\n    if labels_energy is not None:\n        labels_energy = labels_energy.to(energy_pred.device)\n        loss_fct = nn.L1Loss()\n        loss = loss_fct(energy_pred.squeeze(-1), labels_energy)\n    return loss, energy_pred\n</code></pre>"},{"location":"api/#atomgen.models.tokengt","title":"tokengt","text":"<p>Implementation of the TokenGT model.</p>"},{"location":"api/#atomgen.models.tokengt.ParallelBlock","title":"ParallelBlock","text":"<p>               Bases: <code>Module</code></p> <p>Parallel transformer block.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class ParallelBlock(nn.Module):\n    \"\"\"Parallel transformer block.\"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: int = 4,\n        dropout: float = 0.0,\n    ):\n        super().__init__()\n        assert dim % num_heads == 0, (\n            f\"dim {dim} should be divisible by num_heads {num_heads}\"\n        )\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim**-0.5\n        self.mlp_hidden_dim = int(mlp_ratio * dim)\n        self.dropout = dropout\n        self.proj_drop = nn.Dropout(self.dropout)\n\n        self.in_proj_in_dim = dim\n        self.in_proj_out_dim = self.mlp_hidden_dim + 3 * dim\n        self.out_proj_in_dim = self.mlp_hidden_dim + dim\n        self.out_proj_out_dim = 2 * dim\n\n        self.in_split = [self.mlp_hidden_dim] + [dim] * 3\n        self.out_split = [dim] * 2\n\n        self.in_norm = nn.LayerNorm(dim)\n        self.q_norm = nn.LayerNorm(self.head_dim)\n        self.k_norm = nn.LayerNorm(self.head_dim)\n        self.in_proj = nn.Linear(self.in_proj_in_dim, self.in_proj_out_dim, bias=False)\n        self.in_proj = nn.Linear(dim, dim * mlp_ratio)\n        self.act_fn = nn.GELU()\n        self.out_proj = nn.Linear(\n            self.out_proj_in_dim, self.out_proj_out_dim, bias=False\n        )\n\n    def forward(\n        self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward function call for the parallel transformer block.\"\"\"\n        b, n, c = x.shape\n        res = x\n        x = self.in_norm(x)\n\n        x = self.in_proj(self.in_norm(x))\n\n        x, q, k, v = torch.split(x, self.in_split, dim=-1)\n        x = self.act_fn(x)\n        x = self.proj_drop(x)\n\n        q = self.q_norm(q.view(b, n, self.num_heads, self.head_dim).transpose(1, 2))\n        k = self.k_norm(k.view(b, n, self.num_heads, self.head_dim).transpose(1, 2))\n        v = v.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n\n        x_attn = (\n            f.scaled_dot_product_attention(\n                q, k, v, attn_mask=attention_mask, dropout_p=self.dropout\n            )\n            .transpose(1, 2)\n            .reshape(b, n, c)\n        )\n\n        x_mlp, x_attn = self.out_proj(torch.cat([x, x_attn], dim=-1)).split(\n            self.out_split, dim=-1\n        )\n        out: torch.Tensor = x_mlp + x_attn + res\n\n        return out\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.ParallelBlock.forward","title":"forward","text":"<pre><code>forward(x, attention_mask=None)\n</code></pre> <p>Forward function call for the parallel transformer block.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>def forward(\n    self, x: torch.Tensor, attention_mask: Optional[torch.Tensor] = None\n) -&gt; torch.Tensor:\n    \"\"\"Forward function call for the parallel transformer block.\"\"\"\n    b, n, c = x.shape\n    res = x\n    x = self.in_norm(x)\n\n    x = self.in_proj(self.in_norm(x))\n\n    x, q, k, v = torch.split(x, self.in_split, dim=-1)\n    x = self.act_fn(x)\n    x = self.proj_drop(x)\n\n    q = self.q_norm(q.view(b, n, self.num_heads, self.head_dim).transpose(1, 2))\n    k = self.k_norm(k.view(b, n, self.num_heads, self.head_dim).transpose(1, 2))\n    v = v.view(b, n, self.num_heads, self.head_dim).transpose(1, 2)\n\n    x_attn = (\n        f.scaled_dot_product_attention(\n            q, k, v, attn_mask=attention_mask, dropout_p=self.dropout\n        )\n        .transpose(1, 2)\n        .reshape(b, n, c)\n    )\n\n    x_mlp, x_attn = self.out_proj(torch.cat([x, x_attn], dim=-1)).split(\n        self.out_split, dim=-1\n    )\n    out: torch.Tensor = x_mlp + x_attn + res\n\n    return out\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.TransformerConfig","title":"TransformerConfig","text":"<p>               Bases: <code>PretrainedConfig</code></p> <p>Configuration class to store the configuration of a TokenGT model.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class TransformerConfig(PretrainedConfig):  # type: ignore\n    \"\"\"Configuration class to store the configuration of a TokenGT model.\"\"\"\n\n    def __init__(\n        self,\n        vocab_size: int = 123,\n        dim: int = 768,\n        num_heads: int = 12,\n        depth: int = 12,\n        mlp_ratio: int = 4,\n        k: int = 16,\n        sigma: float = 0.03,\n        type_id_dim: int = 64,\n        dropout: float = 0.0,\n        mask_token_id: int = 0,\n        pad_token_id: int = 119,\n        bos_token_id: int = 120,\n        eos_token_id: int = 121,\n        cls_token_id: int = 122,\n        gradient_checkpointing: bool = False,\n        **kwargs: Any,\n    ):\n        super().__init__(**kwargs)\n        self.vocab_size = vocab_size\n        self.dim = dim\n        self.num_heads = num_heads\n        self.depth = depth\n        self.mlp_ratio = mlp_ratio\n        self.k = k\n        self.sigma = sigma\n        self.type_id_dim = type_id_dim\n        self.dropout = dropout\n        self.mask_token_id = mask_token_id\n        self.pad_token_id = pad_token_id\n        self.bos_token_id = bos_token_id\n        self.eos_token_id = eos_token_id\n        self.cls_token_id = cls_token_id\n        self.gradient_checkpointing = gradient_checkpointing\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.TransformerEncoder","title":"TransformerEncoder","text":"<p>               Bases: <code>Module</code></p> <p>Transformer encoder for atom modeling.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class TransformerEncoder(nn.Module):\n    \"\"\"Transformer encoder for atom modeling.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        super().__init__()\n        self.vocab_size = config.vocab_size\n        self.dim = config.dim\n        self.num_heads = config.num_heads\n        self.depth = config.depth\n        self.mlp_ratio = config.mlp_ratio\n        self.k = config.k\n        self.sigma = config.sigma\n        self.type_id_dim = config.type_id_dim\n        self.gradient_checkpointing = config.gradient_checkpointing\n        self.dropout = config.dropout\n        self.metadata_vocab = nn.Embedding(122, 17)\n        vocab_weight = torch.empty(122, 17).fill_(-1.0)\n        vocab_weight[2:-2] = torch.tensor(ATOM_METADATA, dtype=torch.float32)\n        self.metadata_vocab.weight = nn.Parameter(vocab_weight, requires_grad=False)\n        self.node_id = nn.Embedding(1, self.type_id_dim)\n        self.edge_id = nn.Embedding(1, self.type_id_dim)\n        self.embed_proj = nn.Linear(17 + 2 * self.k + self.type_id_dim, self.dim)\n        self.graph = nn.Embedding(1, self.dim)\n        self.distance = nn.Embedding(1, 17)\n        self.distance_norm = nn.LayerNorm(17)\n\n        self.blocks = nn.ModuleList()\n        for _ in range(self.depth):\n            self.blocks.append(\n                ParallelBlock(self.dim, self.num_heads, self.mlp_ratio, self.dropout)\n            )\n\n    def _expand_mask(\n        self,\n        mask: torch.Tensor,\n        dtype: torch.dtype,\n        device: torch.device,\n        tgt_len: Optional[int] = None,\n    ) -&gt; torch.Tensor:\n        bsz, src_len = mask.size()\n        tgt_len = tgt_len if tgt_len is not None else src_len\n\n        expanded_mask = (\n            mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n        )\n\n        inverted_mask: torch.Tensor = 1.0 - expanded_mask\n\n        return inverted_mask.masked_fill(\n            inverted_mask.to(torch.bool), torch.finfo(dtype).min\n        ).to(device)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        node_pe: torch.Tensor,\n        edge_pe: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward function call for the transformer encoder.\"\"\"\n        atom_metadata = self.metadata_vocab(input_ids)  # (B, N, 17)\n\n        node_ids = self.node_id(\n            torch.zeros(\n                node_pe.size(0),\n                node_pe.size(1),\n                dtype=torch.long,\n                device=node_pe.device,\n            )\n        )\n        edge_ids = self.edge_id(\n            torch.zeros(\n                edge_pe.size(0),\n                edge_pe.size(1),\n                dtype=torch.long,\n                device=edge_pe.device,\n            )\n        )\n\n        graph_tokens = self.graph(\n            torch.zeros(node_pe.size(0), 1, dtype=torch.long, device=node_pe.device)\n        )\n\n        nodes = torch.cat([atom_metadata, node_pe, node_ids], dim=-1)\n        distance_embed = self.distance_norm(\n            self.distance(\n                torch.zeros(\n                    edge_pe.size(0),\n                    edge_pe.size(1),\n                    dtype=torch.long,\n                    device=edge_pe.device,\n                )\n            )\n            * edge_pe[:, :, -1:]\n        )\n        edges = torch.cat([distance_embed, edge_pe[:, :, :-1], edge_ids], dim=-1)\n\n        input_embeds: torch.Tensor = self.embed_proj(torch.cat([nodes, edges], dim=1))\n        input_embeds = torch.cat([graph_tokens, input_embeds], dim=1)\n\n        # convert attention mask from long into Boolean and add ones for graph token\n        attention_mask = (\n            torch.cat(\n                [\n                    torch.ones(\n                        attention_mask.size(0),\n                        1,\n                        dtype=torch.bool,\n                        device=attention_mask.device,\n                    ),\n                    attention_mask.bool(),\n                ],\n                dim=1,\n            )\n            if attention_mask is not None\n            else None\n        )\n\n        for blk in self.blocks:\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module: Any) -&gt; Callable[..., Any]:\n                    def custom_forward(*inputs: Any) -&gt; Any:\n                        return module(*inputs)\n\n                    return custom_forward\n\n                input_embeds = checkpoint(\n                    create_custom_forward(blk),\n                    input_embeds,\n                    attention_mask,\n                )\n            else:\n                input_embeds = blk(input_embeds, attention_mask)\n        return input_embeds\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.TransformerEncoder.forward","title":"forward","text":"<pre><code>forward(\n    input_ids, coords, node_pe, edge_pe, attention_mask=None\n)\n</code></pre> <p>Forward function call for the transformer encoder.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    node_pe: torch.Tensor,\n    edge_pe: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Forward function call for the transformer encoder.\"\"\"\n    atom_metadata = self.metadata_vocab(input_ids)  # (B, N, 17)\n\n    node_ids = self.node_id(\n        torch.zeros(\n            node_pe.size(0),\n            node_pe.size(1),\n            dtype=torch.long,\n            device=node_pe.device,\n        )\n    )\n    edge_ids = self.edge_id(\n        torch.zeros(\n            edge_pe.size(0),\n            edge_pe.size(1),\n            dtype=torch.long,\n            device=edge_pe.device,\n        )\n    )\n\n    graph_tokens = self.graph(\n        torch.zeros(node_pe.size(0), 1, dtype=torch.long, device=node_pe.device)\n    )\n\n    nodes = torch.cat([atom_metadata, node_pe, node_ids], dim=-1)\n    distance_embed = self.distance_norm(\n        self.distance(\n            torch.zeros(\n                edge_pe.size(0),\n                edge_pe.size(1),\n                dtype=torch.long,\n                device=edge_pe.device,\n            )\n        )\n        * edge_pe[:, :, -1:]\n    )\n    edges = torch.cat([distance_embed, edge_pe[:, :, :-1], edge_ids], dim=-1)\n\n    input_embeds: torch.Tensor = self.embed_proj(torch.cat([nodes, edges], dim=1))\n    input_embeds = torch.cat([graph_tokens, input_embeds], dim=1)\n\n    # convert attention mask from long into Boolean and add ones for graph token\n    attention_mask = (\n        torch.cat(\n            [\n                torch.ones(\n                    attention_mask.size(0),\n                    1,\n                    dtype=torch.bool,\n                    device=attention_mask.device,\n                ),\n                attention_mask.bool(),\n            ],\n            dim=1,\n        )\n        if attention_mask is not None\n        else None\n    )\n\n    for blk in self.blocks:\n        if self.gradient_checkpointing and self.training:\n\n            def create_custom_forward(module: Any) -&gt; Callable[..., Any]:\n                def custom_forward(*inputs: Any) -&gt; Any:\n                    return module(*inputs)\n\n                return custom_forward\n\n            input_embeds = checkpoint(\n                create_custom_forward(blk),\n                input_embeds,\n                attention_mask,\n            )\n        else:\n            input_embeds = blk(input_embeds, attention_mask)\n    return input_embeds\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.TransformerPreTrainedModel","title":"TransformerPreTrainedModel","text":"<p>               Bases: <code>PreTrainedModel</code></p> <p>Base class for all transformer models.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class TransformerPreTrainedModel(PreTrainedModel):  # type: ignore\n    \"\"\"Base class for all transformer models.\"\"\"\n\n    config_class = TransformerConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"ParallelBlock\"]\n\n    def _set_gradient_checkpointing(\n        self, module: nn.Module, value: bool = False\n    ) -&gt; None:\n        if isinstance(module, (TransformerEncoder)):\n            module.gradient_checkpointing = value\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.TransformerModel","title":"TransformerModel","text":"<p>               Bases: <code>TransformerPreTrainedModel</code></p> <p>Transformer model for atom modeling.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class TransformerModel(TransformerPreTrainedModel):\n    \"\"\"Transformer model for atom modeling.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = TransformerEncoder(config)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; torch.Tensor:\n        \"\"\"Forward function call for the transformer model.\"\"\"\n        out: torch.Tensor = self.encoder(input_ids, coords, attention_mask)\n        return out\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.TransformerModel.forward","title":"forward","text":"<pre><code>forward(input_ids, coords, attention_mask=None)\n</code></pre> <p>Forward function call for the transformer model.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; torch.Tensor:\n    \"\"\"Forward function call for the transformer model.\"\"\"\n    out: torch.Tensor = self.encoder(input_ids, coords, attention_mask)\n    return out\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.TransformerForMaskedAM","title":"TransformerForMaskedAM","text":"<p>               Bases: <code>TransformerPreTrainedModel</code></p> <p>Transformer with an atom modeling head on top for masked atom modeling.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class TransformerForMaskedAM(TransformerPreTrainedModel):\n    \"\"\"Transformer with an atom modeling head on top for masked atom modeling.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = TransformerEncoder(config)\n        self.am_head = nn.Linear(config.dim, config.vocab_size, bias=False)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels: Optional[torch.Tensor] = None,\n        fixed: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n        \"\"\"Forward function call for the masked atom modeling model.\"\"\"\n        hidden_states = self.encoder(input_ids, coords, attention_mask)\n        logits = self.am_head(hidden_states[:, 1 : input_ids.size(1)])\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            logits, labels = logits.view(-1, self.config.vocab_size), labels.view(-1)\n            loss = loss_fct(logits, labels)\n\n        return loss, logits\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.TransformerForMaskedAM.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels=None,\n    fixed=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the masked atom modeling model.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels: Optional[torch.Tensor] = None,\n    fixed: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"Forward function call for the masked atom modeling model.\"\"\"\n    hidden_states = self.encoder(input_ids, coords, attention_mask)\n    logits = self.am_head(hidden_states[:, 1 : input_ids.size(1)])\n\n    loss = None\n    if labels is not None:\n        loss_fct = nn.CrossEntropyLoss()\n        logits, labels = logits.view(-1, self.config.vocab_size), labels.view(-1)\n        loss = loss_fct(logits, labels)\n\n    return loss, logits\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.TransformerForCoordinateAM","title":"TransformerForCoordinateAM","text":"<p>               Bases: <code>TransformerPreTrainedModel</code></p> <p>Transformer with an atom coordinate head on top for coordinate denoising.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class TransformerForCoordinateAM(TransformerPreTrainedModel):\n    \"\"\"Transformer with an atom coordinate head on top for coordinate denoising.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = TransformerEncoder(config)\n        self.coords_head = nn.Linear(config.dim, 3)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels_coords: Optional[torch.Tensor] = None,\n        fixed: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n        \"\"\"Forward function call for the coordinate atom modeling model.\"\"\"\n        hidden_states = self.encoder(input_ids, coords, attention_mask)\n        coords_pred = self.coords_head(hidden_states[:, 1 : input_ids.size(1)])\n\n        loss = None\n        if labels_coords is not None:\n            labels_coords = labels_coords.to(coords_pred.device)\n            loss_fct = nn.L1Loss()\n            loss = loss_fct(coords_pred, labels_coords)\n\n        return loss, coords_pred\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.TransformerForCoordinateAM.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels_coords=None,\n    fixed=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the coordinate atom modeling model.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels_coords: Optional[torch.Tensor] = None,\n    fixed: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"Forward function call for the coordinate atom modeling model.\"\"\"\n    hidden_states = self.encoder(input_ids, coords, attention_mask)\n    coords_pred = self.coords_head(hidden_states[:, 1 : input_ids.size(1)])\n\n    loss = None\n    if labels_coords is not None:\n        labels_coords = labels_coords.to(coords_pred.device)\n        loss_fct = nn.L1Loss()\n        loss = loss_fct(coords_pred, labels_coords)\n\n    return loss, coords_pred\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.InitialStructure2RelaxedStructure","title":"InitialStructure2RelaxedStructure","text":"<p>               Bases: <code>TransformerPreTrainedModel</code></p> <p>Transformer with an coordinate head on top for relaxed structure prediction.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class InitialStructure2RelaxedStructure(TransformerPreTrainedModel):\n    \"\"\"Transformer with an coordinate head on top for relaxed structure prediction.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = TransformerEncoder(config)\n        self.coords_head = nn.Linear(config.dim, 3)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels_coords: Optional[torch.Tensor] = None,\n        fixed: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n        \"\"\"Forward function call.\n\n        Initial structure to relaxed structure model.\n        \"\"\"\n        hidden_states = self.encoder(input_ids, coords, attention_mask)\n        coords_pred = self.coords_head(hidden_states[:, 1 : input_ids.size(1)])\n\n        loss = None\n        if labels_coords is not None:\n            labels_coords = labels_coords.to(coords_pred.device)\n            loss_fct = nn.L1Loss()\n            loss = loss_fct(coords_pred, labels_coords)\n\n        return loss, coords_pred\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.InitialStructure2RelaxedStructure.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels_coords=None,\n    fixed=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call.</p> <p>Initial structure to relaxed structure model.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels_coords: Optional[torch.Tensor] = None,\n    fixed: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"Forward function call.\n\n    Initial structure to relaxed structure model.\n    \"\"\"\n    hidden_states = self.encoder(input_ids, coords, attention_mask)\n    coords_pred = self.coords_head(hidden_states[:, 1 : input_ids.size(1)])\n\n    loss = None\n    if labels_coords is not None:\n        labels_coords = labels_coords.to(coords_pred.device)\n        loss_fct = nn.L1Loss()\n        loss = loss_fct(coords_pred, labels_coords)\n\n    return loss, coords_pred\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.InitialStructure2RelaxedEnergy","title":"InitialStructure2RelaxedEnergy","text":"<p>               Bases: <code>TransformerPreTrainedModel</code></p> <p>Transformer with an energy head on top for relaxed energy prediction.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class InitialStructure2RelaxedEnergy(TransformerPreTrainedModel):\n    \"\"\"Transformer with an energy head on top for relaxed energy prediction.\"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = TransformerEncoder(config)\n        self.energy_norm = nn.LayerNorm(config.dim)\n        self.energy_head = nn.Linear(config.dim, 1, bias=False)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels_energy: Optional[torch.Tensor] = None,\n        fixed: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n        \"\"\"Forward function call for the initial structure to relaxed energy model.\"\"\"\n        hidden_states = self.encoder(input_ids, coords, attention_mask)\n        energy = self.energy_head(self.energy_norm(hidden_states[:, 0])).squeeze(-1)\n\n        loss = None\n        if labels_energy is not None:\n            loss_fct = nn.L1Loss()\n            loss = loss_fct(energy, labels_energy)\n\n        return loss, energy\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.InitialStructure2RelaxedEnergy.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels_energy=None,\n    fixed=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call for the initial structure to relaxed energy model.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels_energy: Optional[torch.Tensor] = None,\n    fixed: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[Optional[torch.Tensor], torch.Tensor]:\n    \"\"\"Forward function call for the initial structure to relaxed energy model.\"\"\"\n    hidden_states = self.encoder(input_ids, coords, attention_mask)\n    energy = self.energy_head(self.energy_norm(hidden_states[:, 0])).squeeze(-1)\n\n    loss = None\n    if labels_energy is not None:\n        loss_fct = nn.L1Loss()\n        loss = loss_fct(energy, labels_energy)\n\n    return loss, energy\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.InitialStructure2RelaxedStructureAndEnergy","title":"InitialStructure2RelaxedStructureAndEnergy","text":"<p>               Bases: <code>TransformerPreTrainedModel</code></p> <p>Initial structure to relaxed structure and energy prediction model.</p> <p>Transformer with an coordinate and energy head on top for relaxed structure and energy prediction.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class InitialStructure2RelaxedStructureAndEnergy(TransformerPreTrainedModel):\n    \"\"\"Initial structure to relaxed structure and energy prediction model.\n\n    Transformer with an coordinate and energy head on top for\n    relaxed structure and energy prediction.\n    \"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = TransformerEncoder(config)\n        self.coords_head = nn.Linear(config.dim, 3)\n        self.energy_norm = nn.LayerNorm(config.dim)\n        self.energy_head = nn.Linear(config.dim, 1, bias=False)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        labels_coords: Optional[torch.Tensor] = None,\n        labels_energy: Optional[torch.Tensor] = None,\n        fixed: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"Forward function call.\n\n        Initial structure to relaxed structure and energy model.\n        \"\"\"\n        hidden_states = self.encoder(input_ids, coords, attention_mask)\n        coords_pred: torch.Tensor = self.coords_head(\n            hidden_states[:, 1 : input_ids.size(1)]\n        )\n        energy: torch.Tensor = self.energy_head(\n            self.energy_norm(hidden_states[:, 0])\n        ).squeeze(-1)\n\n        loss_coords = torch.tensor(0.0, device=input_ids.device)\n        if labels_coords is not None:\n            labels_coords = labels_coords.to(coords_pred.device)\n            loss_fct = nn.L1Loss()\n            loss_coords = loss_fct(coords_pred, labels_coords)\n\n        loss_energy = torch.tensor(0.0, device=input_ids.device)\n        if labels_energy is not None:\n            loss_fct = nn.L1Loss()\n            loss_energy = loss_fct(energy, labels_energy)\n\n        loss = loss_coords + loss_energy\n\n        return loss, (coords_pred, energy)\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.InitialStructure2RelaxedStructureAndEnergy.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    labels_coords=None,\n    labels_energy=None,\n    fixed=None,\n    attention_mask=None,\n)\n</code></pre> <p>Forward function call.</p> <p>Initial structure to relaxed structure and energy model.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    labels_coords: Optional[torch.Tensor] = None,\n    labels_energy: Optional[torch.Tensor] = None,\n    fixed: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n    \"\"\"Forward function call.\n\n    Initial structure to relaxed structure and energy model.\n    \"\"\"\n    hidden_states = self.encoder(input_ids, coords, attention_mask)\n    coords_pred: torch.Tensor = self.coords_head(\n        hidden_states[:, 1 : input_ids.size(1)]\n    )\n    energy: torch.Tensor = self.energy_head(\n        self.energy_norm(hidden_states[:, 0])\n    ).squeeze(-1)\n\n    loss_coords = torch.tensor(0.0, device=input_ids.device)\n    if labels_coords is not None:\n        labels_coords = labels_coords.to(coords_pred.device)\n        loss_fct = nn.L1Loss()\n        loss_coords = loss_fct(coords_pred, labels_coords)\n\n    loss_energy = torch.tensor(0.0, device=input_ids.device)\n    if labels_energy is not None:\n        loss_fct = nn.L1Loss()\n        loss_energy = loss_fct(energy, labels_energy)\n\n    loss = loss_coords + loss_energy\n\n    return loss, (coords_pred, energy)\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.Structure2EnergyAndForces","title":"Structure2EnergyAndForces","text":"<p>               Bases: <code>TransformerPreTrainedModel</code></p> <p>Structure to energy and forces prediction model.</p> <p>Transformer with an energy and forces head on top for energy and forces prediction.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>class Structure2EnergyAndForces(TransformerPreTrainedModel):\n    \"\"\"Structure to energy and forces prediction model.\n\n    Transformer with an energy and forces head on top for energy and forces prediction.\n    \"\"\"\n\n    def __init__(self, config: TransformerConfig):\n        super().__init__(config)\n        self.config = config\n        self.encoder = TransformerEncoder(config)\n        self.force_norm = nn.LayerNorm(config.dim)\n        self.force_head = nn.Linear(config.dim, 3)\n        self.energy_norm = nn.LayerNorm(config.dim)\n        self.formation_energy_head = nn.Linear(config.dim, 1, bias=False)\n\n    def forward(\n        self,\n        input_ids: torch.Tensor,\n        coords: torch.Tensor,\n        forces: Optional[torch.Tensor] = None,\n        total_energy: Optional[torch.Tensor] = None,\n        formation_energy: Optional[torch.Tensor] = None,\n        has_formation_energy: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        node_pe: Optional[torch.Tensor] = None,\n        edge_pe: Optional[torch.Tensor] = None,\n    ) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]]:\n        \"\"\"Forward function call for the structure to energy and forces model.\"\"\"\n        hidden_states = self.encoder(\n            input_ids, coords, attention_mask, node_pe, edge_pe\n        )\n\n        formation_energy_pred: torch.Tensor = self.formation_energy_head(\n            self.energy_norm(hidden_states[:, 0])\n        ).squeeze(-1)\n\n        loss_formation_energy = torch.Tensor(0.0, device=input_ids.device)\n        if formation_energy is not None:\n            loss_fct = nn.L1Loss()\n            loss_formation_energy = loss_fct(\n                formation_energy_pred[has_formation_energy],\n                formation_energy[has_formation_energy],\n            )\n\n        forces_pred: torch.Tensor = self.force_head(\n            self.force_norm(hidden_states[:, 1 : input_ids.size(1)])\n        )\n        loss_forces = torch.Tensor(0.0, device=input_ids.device)\n        if forces is not None and attention_mask is not None:\n            loss_fct = nn.L1Loss()\n            loss_forces = loss_fct(\n                forces_pred[attention_mask[:, 1 : input_ids.size(1)].bool()],\n                forces[attention_mask[:, 1 : input_ids.size(1)].bool()],\n            )\n\n        loss = loss_formation_energy + loss_forces\n\n        return loss, (\n            formation_energy_pred,\n            forces_pred,\n            attention_mask.bool() if attention_mask is not None else attention_mask,\n        )\n</code></pre>"},{"location":"api/#atomgen.models.tokengt.Structure2EnergyAndForces.forward","title":"forward","text":"<pre><code>forward(\n    input_ids,\n    coords,\n    forces=None,\n    total_energy=None,\n    formation_energy=None,\n    has_formation_energy=None,\n    attention_mask=None,\n    node_pe=None,\n    edge_pe=None,\n)\n</code></pre> <p>Forward function call for the structure to energy and forces model.</p> Source code in <code>atomgen/models/tokengt.py</code> <pre><code>def forward(\n    self,\n    input_ids: torch.Tensor,\n    coords: torch.Tensor,\n    forces: Optional[torch.Tensor] = None,\n    total_energy: Optional[torch.Tensor] = None,\n    formation_energy: Optional[torch.Tensor] = None,\n    has_formation_energy: Optional[torch.Tensor] = None,\n    attention_mask: Optional[torch.Tensor] = None,\n    node_pe: Optional[torch.Tensor] = None,\n    edge_pe: Optional[torch.Tensor] = None,\n) -&gt; Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]]:\n    \"\"\"Forward function call for the structure to energy and forces model.\"\"\"\n    hidden_states = self.encoder(\n        input_ids, coords, attention_mask, node_pe, edge_pe\n    )\n\n    formation_energy_pred: torch.Tensor = self.formation_energy_head(\n        self.energy_norm(hidden_states[:, 0])\n    ).squeeze(-1)\n\n    loss_formation_energy = torch.Tensor(0.0, device=input_ids.device)\n    if formation_energy is not None:\n        loss_fct = nn.L1Loss()\n        loss_formation_energy = loss_fct(\n            formation_energy_pred[has_formation_energy],\n            formation_energy[has_formation_energy],\n        )\n\n    forces_pred: torch.Tensor = self.force_head(\n        self.force_norm(hidden_states[:, 1 : input_ids.size(1)])\n    )\n    loss_forces = torch.Tensor(0.0, device=input_ids.device)\n    if forces is not None and attention_mask is not None:\n        loss_fct = nn.L1Loss()\n        loss_forces = loss_fct(\n            forces_pred[attention_mask[:, 1 : input_ids.size(1)].bool()],\n            forces[attention_mask[:, 1 : input_ids.size(1)].bool()],\n        )\n\n    loss = loss_formation_energy + loss_forces\n\n    return loss, (\n        formation_energy_pred,\n        forces_pred,\n        attention_mask.bool() if attention_mask is not None else attention_mask,\n    )\n</code></pre>"},{"location":"user_guide/","title":"AtomGen User Guide","text":"<p>Welcome to the AtomGen User Guide. This document provides comprehensive instructions on how to use all components of the AtomGen library for molecular modeling tasks.</p>"},{"location":"user_guide/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation</li> <li>Quick Start</li> <li>Data Loading</li> <li>Pretraining</li> <li>Fine-tuning</li> <li>Inference</li> <li>Advanced Features</li> <li>Troubleshooting</li> </ol>"},{"location":"user_guide/#installation","title":"Installation","text":"<p>The package can be installed using uv:</p> <pre><code>uv sync\nsource .venv/bin/activate\n</code></pre>"},{"location":"user_guide/#quick-start","title":"Quick Start","text":"<p>Here's a simple example to get you started with AtomGen using a pretrained model to extract features:</p> <pre><code>import torch\nfrom transformers import AutoModel\n\n# Load a pretrained model\nmodel = AutoModel.from_pretrained(\"vector-institute/atomformer-base\",\n                                   trust_remote_code=True)\n\n# Example input data\ninput_ids = torch.randint(0, 50, (1, 10))\ncoords = torch.randn(1, 10, 3)\nattention_mask = torch.ones(1, 10)\n\n# Extract features\nwith torch.no_grad():\n    output = model(input_ids, coords=coords, attention_mask=attention_mask)\n\nprint(output.shape) # Should be (1, 10, 768) for the base model\n</code></pre> <p>This example demonstrates how to load the pretrained AtomFormer model and use it to extract features from molecular data.</p>"},{"location":"user_guide/#data-loading","title":"Data Loading","text":"<p>AtomGen leverages the HuggingFace <code>datasets</code> library for data loading. Here are examples of loading some of the available datasets:</p> <pre><code>from datasets import load_dataset\n\n# Load the S2EF-15M dataset\ns2ef_dataset = load_dataset(\"vector-institute/s2ef-15m\")\n\n# Load ATOM3D SMP dataset\nsmp_dataset = load_dataset(\"vector-institute/atom3d-smp\")\n\n# Load ATOM3D LBA dataset\nlba_dataset = load_dataset(\"vector-institute/atom3d-lba\")\n</code></pre> <p>Dataset structure: - S2EF-15M: Contains 'input_ids' (atomic numbers), 'coords' (3D coordinates), 'forces', 'formation_energy', 'total_energy', and 'has_formation_energy' fields. - ATOM3D datasets: Generally contain 'input_ids', 'coords', and task-specific labels. For example, SMP has 20 regression targets, while LBA has a single binding affinity value.</p> <p>You can inspect the structure of a dataset using:</p> <pre><code>print(dataset['train'].features)\n</code></pre>"},{"location":"user_guide/#pretraining","title":"Pretraining","text":"<p>To pretrain an AtomFormer model, use the <code>pretrain_s2ef.py</code> script. Here's an example of how to use it:</p> <pre><code>python pretrain_s2ef.py \\\n    --seed 42 \\\n    --project \"AtomGen\" \\\n    --name \"s2ef_15m_train_base_10epochs\" \\\n    --output_dir \"./checkpoint\" \\\n    --dataset_dir \"./s2ef_15m\" \\\n    --model_config \"atomgen/models/configs/atomformer-base.json\" \\\n    --tokenizer_json \"atomgen/data/tokenizer.json\" \\\n    --micro_batch_size 8 \\\n    --macro_batch_size 128 \\\n    --num_train_epochs 10 \\\n    --warmup_ratio 0.001 \\\n    --lr_scheduler_type \"cosine\" \\\n    --weight_decay 1.0e-2 \\\n    --max_grad_norm 5.0 \\\n    --learning_rate 3e-4 \\\n    --gradient_checkpointing\n</code></pre> <p>This script handles the complexities of pretraining, including data loading, model initialization, and training loop management.</p>"},{"location":"user_guide/#fine-tuning","title":"Fine-tuning","text":"<p>For fine-tuning on ATOM3D tasks, use the <code>run_atom3d.py</code> script. Here's an example command:</p> <pre><code>python run_atom3d.py \\\n    --model_name_or_path \"vector-institute/atomformer-base\" \\\n    --dataset_name \"vector-institute/atom3d-smp\" \\\n    --output_dir \"./results\" \\\n    --batch_size 32 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs 3 \\\n</code></pre> <p>Key arguments for <code>run_atom3d.py</code>:</p> <ul> <li><code>--model_name_or_path</code>: Pretrained model to start from</li> <li><code>--dataset_name</code>: ATOM3D dataset to use for fine-tuning</li> <li><code>--output_dir</code>: Directory to save results</li> <li><code>--batch_size</code>: Batch size per GPU/CPU for training</li> <li><code>--learning_rate</code>: Initial learning rate</li> <li><code>--num_train_epochs</code>: Total number of training epochs</li> </ul>"},{"location":"user_guide/#inference","title":"Inference","text":"<p>To use a trained model for inference, you can load it directly from the HuggingFace Hub or from a local directory:</p> <pre><code>from transformers import AutoModelForSequenceClassification\nimport torch\n\n# Load from HuggingFace Hub\nmodel = AutoModelForSequenceClassification.from_pretrained(\"vector-institute/atomformer-base-smp\",\n                                                           trust_remote_code=True)\n\n# Or load from a local directory\n# model = AutoModelForSequenceClassification.from_pretrained(\"path/to/your/model/directory\",\n#                                                            trust_remote_code=True)\n\n# Prepare your input data\ninput_ids = torch.randint(0, 50, (1, 10))\ncoords = torch.randn(1, 10, 3)\nattention_mask = torch.ones(1, 10)\n\n# Run inference\nwith torch.no_grad():\n    output = model(input_ids, coords=coords, attention_mask=attention_mask)\n\npredictions = output[1]\nprint(predictions.shape)  # Should be (1, 20) for the SMP task\n</code></pre> <p>This example assumes the model has been fine-tuned on the SMP task. Adjust the model class and output processing based on the specific task you're working with.</p>"},{"location":"user_guide/#advanced-features","title":"Advanced Features","text":""},{"location":"user_guide/#data-collation","title":"Data Collation","text":"<p>The <code>DataCollatorForAtomModeling</code> class handles batching of molecular data. Here's how to use it:</p> <pre><code>from atomgen.data import DataCollatorForAtomModeling\n\ndata_collator = DataCollatorForAtomModeling(\n    mam=True,  # Enable Masked Atom Modeling\n    coords_perturb=0.1,  # Enable coordinate perturbation\n    return_lap_pe=True,  # Return Laplacian Positional Encoding\n)\n</code></pre>"},{"location":"user_guide/#distributed-training","title":"Distributed Training","text":"<p>For multi-GPU training, modify your <code>run_atom3d.py</code> command:</p> <pre><code>python -m torch.distributed.launch --nproc_per_node=4 run_atom3d.py \\\n    --model_name_or_path \"vector-institute/atomformer-base\" \\\n    --dataset_name \"vector-institute/atom3d-smp\" \\\n    --output_dir \"./results\" \\\n    --batch_size 8 \\\n    --learning_rate 5e-5 \\\n    --num_train_epochs 3 \\\n</code></pre>"},{"location":"user_guide/#troubleshooting","title":"Troubleshooting","text":"<p>If you encounter out-of-memory errors, try the following:</p> <ol> <li>Reduce batch size in the script arguments</li> <li>Enable gradient checkpointing (add <code>--gradient_checkpointing</code> to your command)</li> </ol> <p>For more help, please check our GitHub Issues or open a new issue if you can't find a solution to your problem.</p>"}]}